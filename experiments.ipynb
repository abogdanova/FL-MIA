{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(dtype('uint8'), (50000, 32, 32, 3)), (dtype('uint8'), (50000, 1))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download cifar10 data\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "[(x.dtype, x.shape) for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS = 3\n",
    "SIZE = 5000\n",
    "\n",
    "def get_data(source):   \n",
    "    \n",
    "    #idx = np.random.choice(len(source[1]), amount*SHARES, replace=False)\n",
    "    \n",
    "    all_data = (np.array(source[0][:SIZE*CLIENTS] / 255, dtype=np.float32), \n",
    "                tf.keras.utils.to_categorical(source[1][:SIZE*CLIENTS])) \n",
    "    \n",
    "    split_data = []\n",
    "    for s in range(CLIENTS):\n",
    "        start = s*SIZE\n",
    "        end = s*SIZE + SIZE\n",
    "        split_data.append((all_data[0][start:end], all_data[1][start:end]))\n",
    "    \n",
    "    external_data = (np.array(source[0][SIZE*CLIENTS:] / 255, dtype=np.float32), \n",
    "                tf.keras.utils.to_categorical(source[1][SIZE*CLIENTS:])) \n",
    "    \n",
    "    return all_data, split_data, external_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_test_data = (np.array(cifar_test[0] / 255, dtype=np.float32), \n",
    "                tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "\n",
    "cifar_train_data, cifar_train_fed_data, attacker_data = get_data(cifar_train)\n",
    "\n",
    "# retina_test_data, retina_test_fed_data = get_data(retina_test, 3000)\n",
    "# retina_train_data, retina_train_fed_data = get_data(retina_train, 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_train_fed_data[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdwElEQVR4nO2dW4xc13Wm/1WXvjeb3bzfL5IlWbZiWaE1MiLLcoIEiicD2cDAsB8MYWCEwSAGYiB5EDzA2APMgzMY2/DDwAN6LEQeOL4ktmNhIiRxNIYNARPGNCVStChbFEVKpEg2b33v6q6qs/JQRaCl2f/qJru7WvH+P4Bg9V69z1m166w61fuvtZa5O4QQv/6U1toBIURnULALkQkKdiEyQcEuRCYo2IXIBAW7EJlQWc5kM3sEwFcAlAH8L3f/QvT7/b3dPjLUR6yBBGg3bVgEfq5KtZvaBoZGkuPVKl9GLwpuc24rGg1qawbHrJN59fl5OieiUuHPrdngx6zPzSXHzfhrdssicCAf34q0HF1V0dHiMwVWYroV38cma5ip1ZNP4ZaD3czKAP4HgN8FcA7Az8zsKXd/kc0ZGerDn3zyYXI8/sRK7POH8Q8mFti84IE0vH0ftX3g330iOb5100Y6pzk/Q231GrdNXr/KbVOT1Hbh8pX0+Btv0DnmTWobGeHPbfLyeWq7dPbl9LmMX3JF8EHTjL/BoeD+1+fTr7UHc8rBG1IjCNpm8C4RvunX075Eb+reTPtx6G+O0jnL+Rh/P4BT7n7a3ecBfBvAo8s4nhBiFVlOsO8A8PqCn8+1x4QQb0NWfYPOzA6a2REzOzI1k/47Tgix+iwn2M8D2LXg553tsTfh7ofc/YC7Hxjo45tfQojVZTnB/jMA7zCzfWbWBeDjAJ5aGbeEECvNLe/Gu3vDzD4N4O/Rkt6ecPdfLDILVkrvIka756Vyepuz2Qh28IPdW3I4AMDmHXu5bfPW5Hi1zM/VnOO7vpH01gh2YmdqNWq7dPFScvzKVb67P7x+HbV1BbJid28/tZVK6XmhWBqsRyTZWalMbUUlfcwG2aUHAA923Mnl2zpXEezUB7v/Rp53qcSfczM4F2NZOru7Pw3g6eUcQwjRGfQNOiEyQcEuRCYo2IXIBAW7EJmgYBciE5a1G3/TmKHM5IRAPjGSCWOB5AVwqaPUzSWjTdt3U1tPF1muok7nlMt8iRsN7mNtnn/bcGyCJ8KcO3cuOd7dy7INgYH+QHrr6qI2L1WpzYj0VgokqEhMKkjiB3BrdywmDbaOx89VIJAHA2HRgnlOZMUwmSvSAAm6swuRCQp2ITJBwS5EJijYhcgEBbsQmdDR3XgDULL0rntY24skhUQJLVH5ruEtfMd909Zt1MaVBL4rHe2Cz85MU1uUVDExOUFt0+SYO7bvpHM2jmygtp7eHmrzqCwYUVCKJlcugspkKAe18Cx4sdn1ZmQcANDkO+elYBe8GvlPa6sBRTOtUDSCxKASu08HCUO6swuRCQp2ITJBwS5EJijYhcgEBbsQmaBgFyITOiq9OYCC1VbzQEdjmkwgM1S6eqlt6947qK1/gCfJsGSMciWQO8r8/bRciRJJuDRUCmxbN29Ojo+sH6JzqizBp3UyaorkQSMJNN09/HUp6jz5Z26Od8+BB0lUTOotB8kukW57K13KEMi2AAzM/6AuI3GkJOlNCKFgFyITFOxCZIKCXYhMULALkQkKdiEyYVnSm5mdATCJVsG3hrsfCH8fhhLJXmLZSS3SWUEeSCQ9QzyTa3gTz2yrVHnzyUYj3XbJI+knkK6iunulYF5f0HZpz549yfHBgQE6p1rhfkRrXCfZWgBQ7h9Jjt/1nvvpHMzzLMCXjv4/apseu8KPyWStoD1YlEVXRC2qgtfMA2nZSX266BqgGmCg/62Ezv4hd49WWwjxNkAf44XIhOUGuwP4BzP7uZkdXAmHhBCrw3I/xj/o7ufNbDOAH5nZS+7+04W/0H4TOAgAw+t41RYhxOqyrDu7u59v/z8K4AcA/r/dF3c/5O4H3P3AQB/f/BJCrC63HOxm1m9mgzceA/g9ACdWyjEhxMqynI/xWwD8wFpZNhUAf+nufxdNsJKh0p2+u5eC4oUgckfRbPA53bxQYjPQJ6JjNpusTU+Q0RQ9r0BuDLodoRxl0vWwP5WizDz+iasaZOZt376D2rpvvys5vmf/bXTOc8+dpLZ6/15q2zyyidpGz7yUHI+E3kaQ2hZllbHCqIvZQK4RD7IKnfkYXDe3HOzufhrAe251vhCis0h6EyITFOxCZIKCXYhMULALkQkKdiEyobMFJ53LCU2fp/NKRDayQE6KCk7O13m/salx3kfN+9JyXncPl/m6u4MMqkBaKYxLXqWgeOGl195IjvcODNI568r8XAN1fj/YPMKzB7fs3JocjzK59u3ZRW1//+PD1PahDzxIbbuIBHvmOD9eNSiKGfUkbDYCKTiQ7JgoF9VgDR0h6M4uRCYo2IXIBAW7EJmgYBciExTsQmRCh3fjC9Rn03XGolpnrD5duZpuMQQApQpP7og2OWs13oKoQhJQevp4nn4zaiUUJJkM9PDd84sTs9S2cyfZ0R5YR+dMXOZVxarg55q4epHamLqydUe6Rh4A7N7Bd/cP/oePUZsHt6xeT6sCl8+epnPmalyRiZKXyqS+IhC3lHKSfBXlzkS18Bi6swuRCQp2ITJBwS5EJijYhcgEBbsQmaBgFyITOiq9GYByJf3+0qgH0gQrtxUkhMzVeVJCbS7dxgkApmf5kvT2p9suNQMVZGaKS1f0iQG4ePIYtU29dp7abv+3jyTHz5y7QOccPZ6u0wYAO4d5ks/+XbzF1tz0ZHJ8apZLm0WJr/2OrRv5PNIeDACqRILdfw/vVHby6E+ozRtBnbkgOyW0kYSoUpAoFQvIaXRnFyITFOxCZIKCXYhMULALkQkKdiEyQcEuRCYsKr2Z2RMA/gDAqLu/uz02AuA7APYCOAPgY+5+fdGzmcFYBlslkBmIthUl/kTJZo15Xu+uCFKNKpW07xPXr9I50zNc5isCCfD1I0epbf3O3dRGW1sFktHYLF+PYyeeo7bf/wBvCHTPyJbk+NWrfK2i16zo5RIgAumtgbTUd/u77qNzzpOWUQAwfonLnlFrqCLQZ41cc1G9PiNrFQlyS7mz/wWAt4q3jwN4xt3fAeCZ9s9CiLcxiwZ7u9/6tbcMPwrgyfbjJwF8ZIX9EkKsMLf6N/sWd7/xlayLaHV0FUK8jVn2Bp23SszQv7bM7KCZHTGzI1PT/KuSQojV5VaD/ZKZbQOA9v+j7Bfd/ZC7H3D3AwP9vFSUEGJ1udVgfwrAY+3HjwH44cq4I4RYLZYivX0LwMMANprZOQCfA/AFAN81s08BOAuAVwNciDuKBpdJGEyBiDKJItliLpDemoH01lNNCxsTUzN0zsTkOLXNXLpMbdOXx6htxz1c8pqYSGebXZviRRTv/827qa3L+byXz7zO5w2mM+KGhvla1RtBC7Aqb8lUKvHroJfUhxxcx7PoNmzlbaiuX+TPuRwUo4wkMWaLZGAnunMUE4sGu7t/gph+Z7G5Qoi3D/oGnRCZoGAXIhMU7EJkgoJdiExQsAuRCR3u9eZoNOtJWyQzdJXTbhZF+lgAUJ/nGWXjk2l5CgB6e7nE02ykzzfYxzOyrk9y6Wo6kMNmpriPTZbyBGBuLv0txUieQn2Kmqrg6zgW9MUbvZZOgqw7l6dOBVLePx09SW3DG9dT24cfeiA5/q53pouHAgAsyDYLbVxg8ygjjo0HMjWT3gLlTXd2IXJBwS5EJijYhcgEBbsQmaBgFyITFOxCZEJHpTcgKioYSBNElms2ufRTn+XZVbUa7782PcPnzc+npbe+KpeTUHD5ZGaG+z8xPk1t4ySzDQCGNoykDU3e++6FY7yv3LEXf0ltu/beTm2bNqWzyuqBnNSMet9doCUT8JNnD1Pb/Hg6e3D3dl5cifWpAwBEmWjRNRwUnHRyzCKsqMpsQSYoP5oQ4tcJBbsQmaBgFyITFOxCZIKCXYhM6OxuvDu8md6NjVrnWDn9nlT2oM7cLN9xR2Cbnea74HNz6RppvdU+fq4gMaHczavtTs3yBJSJi7x2XddW0nbpCq9pd2H0Cj9ehfs4O82Vi1dfPZs2BBvM1T6e0PLQw++ntj17eM24np70NTIZJCFVylxd8UBdQSlKduFPvCAXiZPrvmVkw9wH3dmFyAQFuxCZoGAXIhMU7EJkgoJdiExQsAuRCUtp//QEgD8AMOru726PfR7AHwK4oQF91t2fXvRYJUN3VzVpK1e4K0x6s+C9qhbkptQCWWtmmtdjq5Gaaza0js4pCi6F9K7nUtP1IHHiwtlz1NZ9Vzo55eoYl96mSYIPADQaPIHm+rWr3DaWrkFXKaVffwAo9wxQ29477qC2Dz30PmrbvWNncnxgiNcatNlt1PZahftfFHytorZMLDsskqPd07aozdRS7ux/AeCRxPiX3f3e9r9FA10IsbYsGuzu/lMA1zrgixBiFVnO3+yfNrPjZvaEmQ2vmEdCiFXhVoP9qwBuA3AvgAsAvsh+0cwOmtkRMzsyNcNb8gohVpdbCnZ3v+TuTW9Vqv8agPuD3z3k7gfc/cBAX9et+imEWCa3FOxmtnC78qMATqyMO0KI1WIp0tu3ADwMYKOZnQPwOQAPm9m9aOXenAHwR0s5WQFDjbTPaQS1yaaupjOUuipBllEvl64q63m7pvkGl3+uj6drk23bxqWavgEuyw3UuRxj27ZS2+Ejz1Mbdqaz3i6N8z3Wos7XvlINLpEgg62op+W8WefyVKXMM+x6e3m7pp07d1Db+nWDyfGhPn684X1cljv7clrKA4BLr52mNg/q63lBpLcgi87InIhFg93dP5EY/vpNn0kIsaboG3RCZIKCXYhMULALkQkKdiEyQcEuRCZ0tODk9FwNh0//KmmrBsUje5tpCaK7i7s/7ePUdtsgl8rKQbHB2nz6G4CNOteg5gPblWvpzDAA2Hnnfmp74We8XdPTf/t/k+PdG9ISFAA0g3ZYbrxFVXeZyz/bhzckxweCDMHXRnnbpV+dfInaeoLCjO+5+67keFeZy1rDg3yttu9/J7WNvvE6tTXneKYlSmn/CyJTA4DZzUtvurMLkQkKdiEyQcEuRCYo2IXIBAW7EJmgYBciEzoqvZVLhsGBdE773BwverhhKC2F1IO+W7MzXPIqlYICi87n1UhhxqtjXOY7/eqr1HbsOM9eGxvjxRzLQbHE0fPpPnA9s7xwyOVrvBhl/xZeYHF4hNcnuIO8Zjt38qJGo1O851xjlvdmO3Oar/G+PXuS4+ncwBb1oMjmyEaejbh+42Zqu3zuDLWVPC33BrVKASY3BnN0ZxciExTsQmSCgl2ITFCwC5EJCnYhMqGju/ElM/SV07u7DeM7oFdqM8nx7qAVz0yDJ3DM1vjO9OxU+lwAcPlyeqe7UubL+NKvfkltJ0++QG316WlqG+oN2l51p9dkNmhrVQ1q+fVt6KO2UhefV66m/SA5TQAAL/FroKfCFZTrV3lC0dFjLybHd+3YTueUg3pxjTr3sV7iNfRmAuWo3EzbLKhBV5AadGwc0J1diGxQsAuRCQp2ITJBwS5EJijYhcgEBbsQmbCU9k+7AHwDrdwBB3DI3b9iZiMAvgNgL1otoD7m7lwDAVC4Y5rU4urv4TLaPJFConpxHtTomq/zemB10rYIACYm0jXS3LlEcuUqb7t0dYzLYfUgcaU3khy70ok8VuFrtb6Ly2tb1/F2WNsGuNRUzKWlz1IxROfsJMkzAACSLAIA03N8/Z/9ybPJ8ZFB3v7pnnfeTm0nT53i5/qnX1Db7vU8wWq4jz83Bilbt+xEmAaAP3X3uwE8AOCPzexuAI8DeMbd3wHgmfbPQoi3KYsGu7tfcPej7ceTAE4C2AHgUQBPtn/tSQAfWS0nhRDL56b+ZjezvQDeC+AwgC3ufqFtuog4RVgIscYsOdjNbADA9wB8xt3fVEnAW/1ok38km9lBMztiZkdqNf5VQyHE6rKkYDezKlqB/k13/357+JKZbWvbtwEYTc1190PufsDdD/T0dPSr+EKIBSwa7GZmaPVjP+nuX1pgegrAY+3HjwH44cq7J4RYKZZyq/0tAJ8E8IKZ3Sia9lkAXwDwXTP7FICzAD622IEaRYHr82nZq6/E5aT6fFq2mAtqhc3UuXR1bZwrhOsHN1HbxuGR5PhgP5dx9uzeTW0vv3KW2ua6+HMrglZC1puuTzc0yOvWFSTrCgD29HM5bO/6Hmq7XkvfRwb7+FrZAJf5ajWexdhT4TJrjWQPfuMv/4bOeeSD76W2V994g9rOnT1PbXdvTNfCA4D+Kmn/RGcAIHJvybj2tmiwu/uz4Ord7yw2Xwjx9kDfoBMiExTsQmSCgl2ITFCwC5EJCnYhMqGj33JpFo7xqbQkNh9IBkUjLULMzwffyCvxTKJr47yV0FAfz1LbuW1bcnx9kBk20L+f2l47d5Hazr5+jtouzfBsOSfFL7u7eKsmOJfe1lf5JdIbZNLN96al1G7WtghAM5BfvcyFqK4gw3HflrR0+Nwpvvb1q69R27s2cLnxzgFexHKkjz/vEiks2eBPCwBbex5HurMLkQkKdiEyQcEuRCYo2IXIBAW7EJmgYBciEzoqvZXLJawbTMtUXuY6Q1c5LScU81wy6uoKilEWQVHJKd5j7eUzrybHxybG6Zzt27ZS2513cFluZiZd3BIAJq5x2WhdT/p5V6uB9BasR9m45GWBXDrQl5bRDPw1iyTASro2CgCgWfB71oE701mM797LC18O9fMMwSq5FgFgdo77MTHDMxW7e9OFO7u7uBQJT58rqDepO7sQuaBgFyITFOxCZIKCXYhMULALkQkd3Y2vlivYPLQ+aXv5/Ot0Hq1bFnTNuVbju5/NSb77bHXeCmny2tXk+Ohw+jkBQFcXX+JSwevklRrc/9oET4Tp8nRyUGF8p7sIWl49e4InhVRKfIf8g++7Jzm+7/ZhOmf6SrJAMQCgaHIfiyAhp4+oMv3BRnczUAXqdf6cS7QnEzDQw1tlse5h83Ved69opv0onKsnurMLkQkKdiEyQcEuRCYo2IXIBAW7EJmgYBciExaV3sxsF4BvoNWS2QEccvevmNnnAfwhgMvtX/2suz8dHWuu3sBro1eStmqFv+80iJzUa1w/aY5xieT0mbSEBgDde3gSxIZ1aVmuNs2lsH/8u7+ltl2bNlKbNbmEUjRmqa2LJLx0FXw9xma5zPfKBZ6QUwvaUG3cNpYcf/+DPAGl2s3XfmaWy1Dm/Lk1WLKU8estyKEKZbkoDaVc5dcqk8uq0RxyfSyr/ROABoA/dfejZjYI4Odm9qO27cvu/t+XcAwhxBqzlF5vFwBcaD+eNLOTAHastmNCiJXlpv5mN7O9AN4L4HB76NNmdtzMnjAz/tUoIcSas+RgN7MBAN8D8Bl3nwDwVQC3AbgXrTv/F8m8g2Z2xMyO1OeCOu9CiFVlScFuZlW0Av2b7v59AHD3S+7edPcCwNcA3J+a6+6H3P2Aux+odnf0q/hCiAUsGuzWqj30dQAn3f1LC8YXtkf5KIATK++eEGKlWMqt9rcAfBLAC2b2fHvsswA+YWb3oiXHnQHwR4sdyOGYIzXN1veQzDYAW/vTdeuGy1yauH72DWqbnuEZVFcneWuoDevS0tDsxHV+vCtpqREAHnjffdS2e4jrP5u6Z6jt1Pl0Db1rtaC+W9Di6YN3873YmTr/s2xgXVpi27yF1+S7co5nPk6M8bZc1Qr331hKWVDTDlErskBeawYSIBp8/Z2crxRkFXogpTKWshv/LNICYqipCyHeXugbdEJkgoJdiExQsAuRCQp2ITJBwS5EJnS24GSphK1d6cJ7d27elhwHgO1D65Lj5y9yyWt0nGdJRZlBtbkgg8rScljhXMprlnuobXSU+7/R+Uuzroe3curtT/sy3+Qy2aZ+frx/s38DtdXqvGBm0ZuW3kpkDQGgf2CQ2iyQvKLXE6W0zZtBUclmIK9Fkl1Aqczvq8yTZpT5WKRtkfqnO7sQmaBgFyITFOxCZIKCXYhMULALkQkKdiEyoaPSW0+5jDtG0lLOpn6e9TY3l5Z4nn/1Ep1zZYIXQ+SZUEBtLpDRSuksu2bB5aTrk9yPs6dfobZd1U3UZqSoJADUGunil+7cx+4qtxmRrgCgEtwrxsfTBSdPvvgSnTN9nWcIBsobnMhQrXlpLSqU64KTeRHYgiN60AcOpIhl+LyiBSHozi5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhM6GxtZzNUKmmZxwuehXRpLF1E8eTZy8lxAJhvcAktki2YzAcAzWZaXCkC6W16ivdlmwmKW1ZKm6mtFqQ2zdaJLciiKweyUKPB5R8r8eddNNNZhy+dOMmP10y/zgDQ3xvdl/h6GJHYonqNLKOsdaooEy2YRiTA9sz0nOB1Zs8rUuR0ZxciExTsQmSCgl2ITFCwC5EJCnYhMmHR3Xgz6wHwUwDd7d//a3f/nJntA/BtABsA/BzAJ92db2Wj9c7Sben3F2/wGmmjV9PJHePTfMc92smMdlsbQa22RjN9vjipgtNd5e+1FvjoRBUAACuTlzTYOe8N/Oju4i22imCN15Ed5tnp9GsJAFXSGqx1Mu5/OUjkYTvdUU0+DxKl5ueDunvBekS198qkfVU5uK78FmrhLeXOPgfgt939PWi1Z37EzB4A8OcAvuzutwO4DuBTN312IUTHWDTYvcWNt+Nq+58D+G0Af90efxLAR1bFQyHEirDU/uzldgfXUQA/AvAKgDF3v/FZ6BwA3u5TCLHmLCnY3b3p7vcC2AngfgB3LfUEZnbQzI6Y2ZHp2ehbbUKI1eSmduPdfQzAjwG8H8B6M7uxs7ATwHky55C7H3D3A/29fLNHCLG6LBrsZrbJzNa3H/cC+F0AJ9EK+n/f/rXHAPxwtZwUQiyfpSTCbAPwpJmV0Xpz+K67/x8zexHAt83svwJ4DsDXl3JCJhjUAylkYjpdxy1MWIh8IDW/AGCeu4HZubQf3UFyBKsvBgBdFf5eOxdIkfV5LruUWR4MbTIEWCDj1OvcjyJI7iiR5Jq+Hi5BNZt8PSKhqRm0cmqSa6ReD6TNKNklcMSDe2cjeD1ZnbxyIJcakbCjhVo02N39OID3JsZPo/X3uxDiXwH6Bp0QmaBgFyITFOxCZIKCXYhMULALkQkWZYet+MnMLgM42/5xI4Cg30/HkB9vRn68mX9tfuxx92TvsI4G+5tObHbE3Q+sycnlh/zI0A99jBciExTsQmTCWgb7oTU890Lkx5uRH2/m18aPNfubXQjRWfQxXohMWJNgN7NHzOyXZnbKzB5fCx/afpwxsxfM7HkzO9LB8z5hZqNmdmLB2IiZ/cjMXm7/P7xGfnzezM631+R5M/twB/zYZWY/NrMXzewXZvYn7fGOrkngR0fXxMx6zOyfzexY24//0h7fZ2aH23HzHTPruqkDu3tH/wEoo1XWaj+ALgDHANzdaT/avpwBsHENzvsQgPsAnFgw9t8APN5+/DiAP18jPz4P4M86vB7bANzXfjwI4FcA7u70mgR+dHRN0OrYNtB+XAVwGMADAL4L4OPt8f8J4D/ezHHX4s5+P4BT7n7aW6Wnvw3g0TXwY81w958CuPaW4UfRKtwJdKiAJ/Gj47j7BXc/2n48iVZxlB3o8JoEfnQUb7HiRV7XIth3AHh9wc9rWazSAfyDmf3czA6ukQ832OLuF9qPLwLYsoa+fNrMjrc/5q/6nxMLMbO9aNVPOIw1XJO3+AF0eE1Wo8hr7ht0D7r7fQB+H8Afm9lDa+0Q0HpnR1ycZTX5KoDb0OoRcAHAFzt1YjMbAPA9AJ9x9zf1s+7kmiT86Pia+DKKvDLWItjPA9i14GdarHK1cffz7f9HAfwAa1t555KZbQOA9v+ja+GEu19qX2gFgK+hQ2tiZlW0Auyb7v799nDH1yTlx1qtSfvcN13klbEWwf4zAO9o7yx2Afg4gKc67YSZ9ZvZ4I3HAH4PwIl41qryFFqFO4E1LOB5I7jafBQdWBMzM7RqGJ509y8tMHV0TZgfnV6TVSvy2qkdxrfsNn4YrZ3OVwD8pzXyYT9aSsAxAL/opB8AvoXWx8E6Wn97fQqtnnnPAHgZwD8CGFkjP/43gBcAHEcr2LZ1wI8H0fqIfhzA8+1/H+70mgR+dHRNAPwGWkVcj6P1xvKfF1yz/wzgFIC/AtB9M8fVN+iEyITcN+iEyAYFuxCZoGAXIhMU7EJkgoJdiExQsAuRCQp2ITJBwS5EJvwLzD05Za3lVj0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(cifar_train_fed_data[2][0][-4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32,(5, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        #tf.keras.layers.Conv2D(32,(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        #tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Conv2D(64, (5, 5), activation=\"relu\", padding=\"same\"),\n",
    "        #tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        #tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        #tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "#     def loss_fn(y_true, y_pred):\n",
    "#         return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "#         y_true, y_pred))\n",
    "    \n",
    "    \n",
    "    model.compile(\n",
    "      loss=tf.keras.losses.categorical_crossentropy,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/36\n",
      "15000/15000 [==============================] - 41s 3ms/sample - loss: 1.6504 - categorical_accuracy: 0.4033 - val_loss: 1.3980 - val_categorical_accuracy: 0.4984\n",
      "Epoch 2/36\n",
      "15000/15000 [==============================] - 40s 3ms/sample - loss: 1.2671 - categorical_accuracy: 0.5487 - val_loss: 1.2085 - val_categorical_accuracy: 0.5669\n",
      "Epoch 3/36\n",
      "15000/15000 [==============================] - 38s 3ms/sample - loss: 1.0512 - categorical_accuracy: 0.6215 - val_loss: 1.2016 - val_categorical_accuracy: 0.5791\n",
      "Epoch 4/36\n",
      "15000/15000 [==============================] - 40s 3ms/sample - loss: 0.8741 - categorical_accuracy: 0.6917 - val_loss: 1.1115 - val_categorical_accuracy: 0.6254\n",
      "Epoch 5/36\n",
      "15000/15000 [==============================] - 37s 2ms/sample - loss: 0.6821 - categorical_accuracy: 0.7599 - val_loss: 1.1041 - val_categorical_accuracy: 0.6338\n",
      "Epoch 6/36\n",
      "15000/15000 [==============================] - 37s 2ms/sample - loss: 0.4978 - categorical_accuracy: 0.8251 - val_loss: 1.2559 - val_categorical_accuracy: 0.6292\n",
      "Epoch 7/36\n",
      "15000/15000 [==============================] - 38s 3ms/sample - loss: 0.3189 - categorical_accuracy: 0.8911 - val_loss: 1.5324 - val_categorical_accuracy: 0.6084\n",
      "Epoch 8/36\n",
      "15000/15000 [==============================] - 39s 3ms/sample - loss: 0.1854 - categorical_accuracy: 0.9401 - val_loss: 1.6345 - val_categorical_accuracy: 0.6232\n",
      "Epoch 9/36\n",
      "15000/15000 [==============================] - 40s 3ms/sample - loss: 0.1310 - categorical_accuracy: 0.9584 - val_loss: 1.7773 - val_categorical_accuracy: 0.6242\n",
      "Epoch 10/36\n",
      "15000/15000 [==============================] - 39s 3ms/sample - loss: 0.1074 - categorical_accuracy: 0.9641 - val_loss: 2.0300 - val_categorical_accuracy: 0.6148\n",
      "Epoch 11/36\n",
      "15000/15000 [==============================] - 39s 3ms/sample - loss: 0.0865 - categorical_accuracy: 0.9722 - val_loss: 2.2109 - val_categorical_accuracy: 0.6199\n",
      "Epoch 12/36\n",
      "15000/15000 [==============================] - 35s 2ms/sample - loss: 0.0837 - categorical_accuracy: 0.9723 - val_loss: 2.3362 - val_categorical_accuracy: 0.6131\n",
      "Epoch 13/36\n",
      "15000/15000 [==============================] - 37s 2ms/sample - loss: 0.0583 - categorical_accuracy: 0.9822 - val_loss: 2.5163 - val_categorical_accuracy: 0.6152\n",
      "Epoch 14/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0690 - categorical_accuracy: 0.9781 - val_loss: 2.5315 - val_categorical_accuracy: 0.6210\n",
      "Epoch 15/36\n",
      "15000/15000 [==============================] - 38s 3ms/sample - loss: 0.0705 - categorical_accuracy: 0.9794 - val_loss: 2.5733 - val_categorical_accuracy: 0.6170\n",
      "Epoch 16/36\n",
      "15000/15000 [==============================] - 38s 3ms/sample - loss: 0.0584 - categorical_accuracy: 0.9819 - val_loss: 2.6380 - val_categorical_accuracy: 0.6092\n",
      "Epoch 17/36\n",
      "15000/15000 [==============================] - 37s 2ms/sample - loss: 0.0580 - categorical_accuracy: 0.9821 - val_loss: 2.8978 - val_categorical_accuracy: 0.6057\n",
      "Epoch 18/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0582 - categorical_accuracy: 0.9807 - val_loss: 2.8335 - val_categorical_accuracy: 0.6094\n",
      "Epoch 19/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0675 - categorical_accuracy: 0.9792 - val_loss: 2.9163 - val_categorical_accuracy: 0.6068\n",
      "Epoch 20/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0523 - categorical_accuracy: 0.9842 - val_loss: 3.0065 - val_categorical_accuracy: 0.5946\n",
      "Epoch 21/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0420 - categorical_accuracy: 0.9859 - val_loss: 3.0636 - val_categorical_accuracy: 0.6152\n",
      "Epoch 22/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0257 - categorical_accuracy: 0.9931 - val_loss: 3.2611 - val_categorical_accuracy: 0.5963\n",
      "Epoch 23/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0675 - categorical_accuracy: 0.9791 - val_loss: 3.3928 - val_categorical_accuracy: 0.6069\n",
      "Epoch 24/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0477 - categorical_accuracy: 0.9847 - val_loss: 3.6397 - val_categorical_accuracy: 0.5968\n",
      "Epoch 25/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0633 - categorical_accuracy: 0.9817 - val_loss: 3.3215 - val_categorical_accuracy: 0.6047\n",
      "Epoch 26/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0283 - categorical_accuracy: 0.9904 - val_loss: 3.6127 - val_categorical_accuracy: 0.5992\n",
      "Epoch 27/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0342 - categorical_accuracy: 0.9887 - val_loss: 3.5899 - val_categorical_accuracy: 0.6070\n",
      "Epoch 28/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0355 - categorical_accuracy: 0.9882 - val_loss: 3.7348 - val_categorical_accuracy: 0.5942\n",
      "Epoch 29/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0838 - categorical_accuracy: 0.9747 - val_loss: 3.4490 - val_categorical_accuracy: 0.5963\n",
      "Epoch 30/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0423 - categorical_accuracy: 0.9870 - val_loss: 3.5481 - val_categorical_accuracy: 0.6125\n",
      "Epoch 31/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0203 - categorical_accuracy: 0.9940 - val_loss: 3.9814 - val_categorical_accuracy: 0.6020\n",
      "Epoch 32/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0527 - categorical_accuracy: 0.9848 - val_loss: 3.9001 - val_categorical_accuracy: 0.5838\n",
      "Epoch 33/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0450 - categorical_accuracy: 0.9880 - val_loss: 3.9359 - val_categorical_accuracy: 0.5935\n",
      "Epoch 34/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0374 - categorical_accuracy: 0.9887 - val_loss: 4.1933 - val_categorical_accuracy: 0.5873\n",
      "Epoch 35/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0339 - categorical_accuracy: 0.9899 - val_loss: 4.0271 - val_categorical_accuracy: 0.5934\n",
      "Epoch 36/36\n",
      "15000/15000 [==============================] - 36s 2ms/sample - loss: 0.0427 - categorical_accuracy: 0.9875 - val_loss: 4.0118 - val_categorical_accuracy: 0.6040\n"
     ]
    }
   ],
   "source": [
    "centralized_model = create_compiled_keras_model()\n",
    "history_callback = centralized_model.fit(cifar_train_data[0], cifar_train_data[1], validation_data=cifar_test_data, batch_size=32, epochs=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_model.save(\"models/centralized.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "try:\n",
    "    with open('Logs/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar, Centralized, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 1.9542 - categorical_accuracy: 0.2820 - val_loss: 1.7007 - val_categorical_accuracy: 0.3844\n",
      "Epoch 2/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 1.5230 - categorical_accuracy: 0.4450 - val_loss: 1.4461 - val_categorical_accuracy: 0.4714\n",
      "Epoch 3/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 1.3119 - categorical_accuracy: 0.5244 - val_loss: 1.4194 - val_categorical_accuracy: 0.4817\n",
      "Epoch 4/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 1.1518 - categorical_accuracy: 0.5800 - val_loss: 1.4005 - val_categorical_accuracy: 0.5119\n",
      "Epoch 5/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.9430 - categorical_accuracy: 0.6670 - val_loss: 1.5534 - val_categorical_accuracy: 0.4974\n",
      "Epoch 6/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.7536 - categorical_accuracy: 0.7334 - val_loss: 1.4333 - val_categorical_accuracy: 0.5389\n",
      "Epoch 7/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.5556 - categorical_accuracy: 0.8068 - val_loss: 1.4302 - val_categorical_accuracy: 0.5567\n",
      "Epoch 8/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.3724 - categorical_accuracy: 0.8778 - val_loss: 1.7458 - val_categorical_accuracy: 0.5402\n",
      "Epoch 9/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.2328 - categorical_accuracy: 0.9288 - val_loss: 2.0106 - val_categorical_accuracy: 0.5097\n",
      "Epoch 10/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.1456 - categorical_accuracy: 0.9588 - val_loss: 2.0361 - val_categorical_accuracy: 0.5527\n",
      "Epoch 11/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0608 - categorical_accuracy: 0.9850 - val_loss: 2.4480 - val_categorical_accuracy: 0.5333\n",
      "Epoch 12/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0730 - categorical_accuracy: 0.9794 - val_loss: 2.5155 - val_categorical_accuracy: 0.5445\n",
      "Epoch 13/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0542 - categorical_accuracy: 0.9848 - val_loss: 2.4842 - val_categorical_accuracy: 0.5430\n",
      "Epoch 14/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0393 - categorical_accuracy: 0.9898 - val_loss: 2.7407 - val_categorical_accuracy: 0.5352\n",
      "Epoch 15/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0450 - categorical_accuracy: 0.9876 - val_loss: 2.6825 - val_categorical_accuracy: 0.5400\n",
      "Epoch 16/36\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.0998 - categorical_accuracy: 0.9698 - val_loss: 2.6621 - val_categorical_accuracy: 0.5508\n",
      "Epoch 17/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0428 - categorical_accuracy: 0.9882 - val_loss: 2.8516 - val_categorical_accuracy: 0.5417\n",
      "Epoch 18/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0401 - categorical_accuracy: 0.9892 - val_loss: 3.0880 - val_categorical_accuracy: 0.5271\n",
      "Epoch 19/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0336 - categorical_accuracy: 0.9904 - val_loss: 3.1828 - val_categorical_accuracy: 0.5289\n",
      "Epoch 20/36\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 0.0656 - categorical_accuracy: 0.9802 - val_loss: 2.9413 - val_categorical_accuracy: 0.5285\n",
      "Epoch 21/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0531 - categorical_accuracy: 0.9844 - val_loss: 3.1371 - val_categorical_accuracy: 0.5361\n",
      "Epoch 22/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.0392 - categorical_accuracy: 0.9888 - val_loss: 3.3258 - val_categorical_accuracy: 0.5229\n",
      "Epoch 23/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0684 - categorical_accuracy: 0.9790 - val_loss: 3.1664 - val_categorical_accuracy: 0.5306\n",
      "Epoch 24/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0207 - categorical_accuracy: 0.9952 - val_loss: 3.4495 - val_categorical_accuracy: 0.5302\n",
      "Epoch 25/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0153 - categorical_accuracy: 0.9964 - val_loss: 3.7348 - val_categorical_accuracy: 0.5255\n",
      "Epoch 26/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0401 - categorical_accuracy: 0.9896 - val_loss: 3.5862 - val_categorical_accuracy: 0.5334\n",
      "Epoch 27/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0189 - categorical_accuracy: 0.9938 - val_loss: 3.6815 - val_categorical_accuracy: 0.5249\n",
      "Epoch 28/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0520 - categorical_accuracy: 0.9842 - val_loss: 3.7682 - val_categorical_accuracy: 0.5243\n",
      "Epoch 29/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0344 - categorical_accuracy: 0.9894 - val_loss: 3.5126 - val_categorical_accuracy: 0.5287\n",
      "Epoch 30/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0079 - categorical_accuracy: 0.9980 - val_loss: 3.8375 - val_categorical_accuracy: 0.5417\n",
      "Epoch 31/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0031 - categorical_accuracy: 0.9988 - val_loss: 3.8883 - val_categorical_accuracy: 0.5390\n",
      "Epoch 32/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0195 - categorical_accuracy: 0.9946 - val_loss: 4.1683 - val_categorical_accuracy: 0.5329\n",
      "Epoch 33/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0616 - categorical_accuracy: 0.9818 - val_loss: 3.6567 - val_categorical_accuracy: 0.5103\n",
      "Epoch 34/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.0412 - categorical_accuracy: 0.9858 - val_loss: 3.9035 - val_categorical_accuracy: 0.5156\n",
      "Epoch 35/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0515 - categorical_accuracy: 0.9828 - val_loss: 3.9987 - val_categorical_accuracy: 0.5301\n",
      "Epoch 36/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0106 - categorical_accuracy: 0.9966 - val_loss: 4.1611 - val_categorical_accuracy: 0.5381\n"
     ]
    }
   ],
   "source": [
    "single_model0 = create_compiled_keras_model()\n",
    "history_callback0 = single_model0.fit(cifar_train_fed_data[0][0], cifar_train_fed_data[0][1], validation_data=cifar_test_data, batch_size=32, epochs=36)\n",
    "single_model0.save('models/ind0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 2.0256 - categorical_accuracy: 0.2418 - val_loss: 1.7465 - val_categorical_accuracy: 0.3599\n",
      "Epoch 2/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.6114 - categorical_accuracy: 0.4114 - val_loss: 1.6765 - val_categorical_accuracy: 0.4082\n",
      "Epoch 3/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.4085 - categorical_accuracy: 0.4914 - val_loss: 1.4323 - val_categorical_accuracy: 0.4859\n",
      "Epoch 4/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.2058 - categorical_accuracy: 0.5690 - val_loss: 1.4569 - val_categorical_accuracy: 0.4911\n",
      "Epoch 5/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.0626 - categorical_accuracy: 0.6194 - val_loss: 1.4098 - val_categorical_accuracy: 0.5073\n",
      "Epoch 6/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.8889 - categorical_accuracy: 0.6844 - val_loss: 1.4469 - val_categorical_accuracy: 0.5042\n",
      "Epoch 7/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.6973 - categorical_accuracy: 0.7512 - val_loss: 1.4888 - val_categorical_accuracy: 0.5283\n",
      "Epoch 8/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.4963 - categorical_accuracy: 0.8264 - val_loss: 1.7206 - val_categorical_accuracy: 0.5267\n",
      "Epoch 9/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.3557 - categorical_accuracy: 0.8776 - val_loss: 1.8015 - val_categorical_accuracy: 0.5210\n",
      "Epoch 10/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.2496 - categorical_accuracy: 0.9168 - val_loss: 1.9786 - val_categorical_accuracy: 0.5267\n",
      "Epoch 11/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.1372 - categorical_accuracy: 0.9588 - val_loss: 2.5817 - val_categorical_accuracy: 0.5079\n",
      "Epoch 12/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.1222 - categorical_accuracy: 0.9628 - val_loss: 2.5287 - val_categorical_accuracy: 0.5303\n",
      "Epoch 13/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0699 - categorical_accuracy: 0.9812 - val_loss: 2.6785 - val_categorical_accuracy: 0.5353\n",
      "Epoch 14/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0510 - categorical_accuracy: 0.9846 - val_loss: 2.9699 - val_categorical_accuracy: 0.5191\n",
      "Epoch 15/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0751 - categorical_accuracy: 0.9820 - val_loss: 2.8482 - val_categorical_accuracy: 0.5009\n",
      "Epoch 16/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0927 - categorical_accuracy: 0.9722 - val_loss: 2.9642 - val_categorical_accuracy: 0.5130\n",
      "Epoch 17/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0689 - categorical_accuracy: 0.9806 - val_loss: 2.9902 - val_categorical_accuracy: 0.5186\n",
      "Epoch 18/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0601 - categorical_accuracy: 0.9816 - val_loss: 3.1546 - val_categorical_accuracy: 0.5257\n",
      "Epoch 19/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0306 - categorical_accuracy: 0.9916 - val_loss: 3.2751 - val_categorical_accuracy: 0.5329\n",
      "Epoch 20/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0248 - categorical_accuracy: 0.9946 - val_loss: 3.6186 - val_categorical_accuracy: 0.5240\n",
      "Epoch 21/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0044 - categorical_accuracy: 1.0000 - val_loss: 3.5844 - val_categorical_accuracy: 0.5351\n",
      "Epoch 22/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0024 - categorical_accuracy: 0.9998 - val_loss: 3.7913 - val_categorical_accuracy: 0.5369\n",
      "Epoch 23/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0028 - categorical_accuracy: 0.9996 - val_loss: 3.7214 - val_categorical_accuracy: 0.5425\n",
      "Epoch 24/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 8.5756e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7896 - val_categorical_accuracy: 0.5449\n",
      "Epoch 25/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 5.4268e-04 - categorical_accuracy: 1.0000 - val_loss: 3.8592 - val_categorical_accuracy: 0.5438\n",
      "Epoch 26/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 3.6305e-04 - categorical_accuracy: 1.0000 - val_loss: 3.9096 - val_categorical_accuracy: 0.5442\n",
      "Epoch 27/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 3.0403e-04 - categorical_accuracy: 1.0000 - val_loss: 3.9681 - val_categorical_accuracy: 0.5434\n",
      "Epoch 28/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 2.6713e-04 - categorical_accuracy: 1.0000 - val_loss: 4.0039 - val_categorical_accuracy: 0.5442\n",
      "Epoch 29/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 2.3226e-04 - categorical_accuracy: 1.0000 - val_loss: 4.0451 - val_categorical_accuracy: 0.5453\n",
      "Epoch 30/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 2.0585e-04 - categorical_accuracy: 1.0000 - val_loss: 4.0798 - val_categorical_accuracy: 0.5441\n",
      "Epoch 31/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.8237e-04 - categorical_accuracy: 1.0000 - val_loss: 4.1217 - val_categorical_accuracy: 0.5447\n",
      "Epoch 32/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.6415e-04 - categorical_accuracy: 1.0000 - val_loss: 4.1621 - val_categorical_accuracy: 0.5442\n",
      "Epoch 33/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.4889e-04 - categorical_accuracy: 1.0000 - val_loss: 4.1909 - val_categorical_accuracy: 0.5439\n",
      "Epoch 34/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.3840e-04 - categorical_accuracy: 1.0000 - val_loss: 4.2288 - val_categorical_accuracy: 0.5446\n",
      "Epoch 35/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.1954e-04 - categorical_accuracy: 1.0000 - val_loss: 4.2635 - val_categorical_accuracy: 0.5439\n",
      "Epoch 36/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.0896e-04 - categorical_accuracy: 1.0000 - val_loss: 4.2970 - val_categorical_accuracy: 0.5436\n"
     ]
    }
   ],
   "source": [
    "single_model1 = create_compiled_keras_model()\n",
    "history_callback1 = single_model1.fit(cifar_train_fed_data[1][0], cifar_train_fed_data[1][1], validation_data=cifar_test_data, batch_size=32, epochs=36)\n",
    "single_model1.save('models/ind1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 1.9030 - categorical_accuracy: 0.3006 - val_loss: 1.6718 - val_categorical_accuracy: 0.3894\n",
      "Epoch 2/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.5808 - categorical_accuracy: 0.4212 - val_loss: 1.4990 - val_categorical_accuracy: 0.4661\n",
      "Epoch 3/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.3553 - categorical_accuracy: 0.5126 - val_loss: 1.3616 - val_categorical_accuracy: 0.5092\n",
      "Epoch 4/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.1615 - categorical_accuracy: 0.5838 - val_loss: 1.3955 - val_categorical_accuracy: 0.5006\n",
      "Epoch 5/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.9671 - categorical_accuracy: 0.6588 - val_loss: 1.3753 - val_categorical_accuracy: 0.5202\n",
      "Epoch 6/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.7888 - categorical_accuracy: 0.7212 - val_loss: 1.4034 - val_categorical_accuracy: 0.5434\n",
      "Epoch 7/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.5908 - categorical_accuracy: 0.7980 - val_loss: 1.5480 - val_categorical_accuracy: 0.5305\n",
      "Epoch 8/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.4045 - categorical_accuracy: 0.8664 - val_loss: 1.7865 - val_categorical_accuracy: 0.5261\n",
      "Epoch 9/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.2538 - categorical_accuracy: 0.9204 - val_loss: 1.9686 - val_categorical_accuracy: 0.5434\n",
      "Epoch 10/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.1746 - categorical_accuracy: 0.9460 - val_loss: 2.2343 - val_categorical_accuracy: 0.5387\n",
      "Epoch 11/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.1005 - categorical_accuracy: 0.9756 - val_loss: 2.2393 - val_categorical_accuracy: 0.5536\n",
      "Epoch 12/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0573 - categorical_accuracy: 0.9862 - val_loss: 2.3988 - val_categorical_accuracy: 0.5459\n",
      "Epoch 13/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0335 - categorical_accuracy: 0.9942 - val_loss: 2.6178 - val_categorical_accuracy: 0.5530\n",
      "Epoch 14/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0177 - categorical_accuracy: 0.9982 - val_loss: 2.7409 - val_categorical_accuracy: 0.5548\n",
      "Epoch 15/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0849 - categorical_accuracy: 0.9740 - val_loss: 2.5801 - val_categorical_accuracy: 0.5288\n",
      "Epoch 16/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0622 - categorical_accuracy: 0.9814 - val_loss: 2.6354 - val_categorical_accuracy: 0.5405\n",
      "Epoch 17/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0319 - categorical_accuracy: 0.9930 - val_loss: 2.9104 - val_categorical_accuracy: 0.5406\n",
      "Epoch 18/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0625 - categorical_accuracy: 0.9810 - val_loss: 2.9321 - val_categorical_accuracy: 0.5194\n",
      "Epoch 19/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.1159 - categorical_accuracy: 0.9676 - val_loss: 2.8769 - val_categorical_accuracy: 0.5332\n",
      "Epoch 20/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0338 - categorical_accuracy: 0.9904 - val_loss: 3.0117 - val_categorical_accuracy: 0.5373\n",
      "Epoch 21/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0276 - categorical_accuracy: 0.9916 - val_loss: 3.0342 - val_categorical_accuracy: 0.5436\n",
      "Epoch 22/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0050 - categorical_accuracy: 0.9988 - val_loss: 3.1507 - val_categorical_accuracy: 0.5529\n",
      "Epoch 23/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0168 - categorical_accuracy: 0.9960 - val_loss: 3.5059 - val_categorical_accuracy: 0.5474\n",
      "Epoch 24/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0027 - categorical_accuracy: 0.9998 - val_loss: 3.4404 - val_categorical_accuracy: 0.5543\n",
      "Epoch 25/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 5.3866e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5066 - val_categorical_accuracy: 0.5570\n",
      "Epoch 26/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 3.1860e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5672 - val_categorical_accuracy: 0.5584\n",
      "Epoch 27/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 2.5130e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6190 - val_categorical_accuracy: 0.5598\n",
      "Epoch 28/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 2.4952e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6646 - val_categorical_accuracy: 0.5607\n",
      "Epoch 29/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.8645e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6969 - val_categorical_accuracy: 0.5606\n",
      "Epoch 30/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.5152e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7383 - val_categorical_accuracy: 0.5612\n",
      "Epoch 31/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.3071e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7748 - val_categorical_accuracy: 0.5618\n",
      "Epoch 32/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.1655e-04 - categorical_accuracy: 1.0000 - val_loss: 3.8049 - val_categorical_accuracy: 0.5619\n",
      "Epoch 33/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.0334e-04 - categorical_accuracy: 1.0000 - val_loss: 3.8395 - val_categorical_accuracy: 0.5614\n",
      "Epoch 34/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 9.3850e-05 - categorical_accuracy: 1.0000 - val_loss: 3.8664 - val_categorical_accuracy: 0.5620\n",
      "Epoch 35/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 8.3524e-05 - categorical_accuracy: 1.0000 - val_loss: 3.8967 - val_categorical_accuracy: 0.5624\n",
      "Epoch 36/36\n",
      "5000/5000 [==============================] - 19s 4ms/sample - loss: 7.5841e-05 - categorical_accuracy: 1.0000 - val_loss: 3.9253 - val_categorical_accuracy: 0.5621\n"
     ]
    }
   ],
   "source": [
    "single_model2 = create_compiled_keras_model()\n",
    "history_callback2 = single_model2.fit(cifar_train_fed_data[2][0], cifar_train_fed_data[2][1], validation_data=cifar_test_data, batch_size=32, epochs=36)\n",
    "single_model2.save('models/ind2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Single party 0, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Ind0Train = {}\".format(history_callback0.history[\"loss\"]), file=log)\n",
    "        print(\"Ind0Test = {}\".format(history_callback0.history[\"val_loss\"]), file=log)\n",
    "        print(\"Ind0Accuracy = {}\".format(history_callback0.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingAvTrain = (np.array(history_callback0.history[\"loss\"]) \n",
    " + np.array(history_callback1.history[\"loss\"]) \n",
    " + np.array(history_callback2.history[\"loss\"])) / 3\n",
    "SingAvTest = (np.array(history_callback0.history[\"val_loss\"]) \n",
    " + np.array(history_callback1.history[\"val_loss\"]) \n",
    " + np.array(history_callback2.history[\"val_loss\"])) / 3\n",
    "SingAvAcc = (np.array(history_callback0.history[\"val_categorical_accuracy\"]) \n",
    " + np.array(history_callback1.history[\"val_categorical_accuracy\"]) \n",
    " + np.array(history_callback2.history[\"val_categorical_accuracy\"])) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, SingleAverage, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"SingAvTrain = {}\".format(SingAvTrain), file=log)\n",
    "        print(\"SingAvTest = {}\".format(SingAvTest), file=log)\n",
    "        print(\"SingAvAcc = {}\".format(SingAvAcc), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 483us/sample\n"
     ]
    }
   ],
   "source": [
    "probabilities0 = single_model0.predict_proba(cifar_test_data[0], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 426us/sample\n"
     ]
    }
   ],
   "source": [
    "probabilities1 = single_model1.predict_proba(cifar_test_data[0], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 3s 622us/sample\n"
     ]
    }
   ],
   "source": [
    "probabilities2 = single_model2.predict_proba(cifar_test_data[0], batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = (probabilities0 + probabilities1 + probabilities2) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47005984\n"
     ]
    }
   ],
   "source": [
    "losses = tf.keras.losses.categorical_crossentropy(cifar_train_data[1], probs, from_logits=False)\n",
    "print(np.mean(losses.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.785349\n"
     ]
    }
   ],
   "source": [
    "val_loss = tf.keras.losses.categorical_crossentropy(cifar_test_data[1], probs, from_logits=False)\n",
    "print(np.mean(val_loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6052"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = probs.argmax(axis=1)\n",
    "y_true = cifar_test_data[1].argmax(axis=1)\n",
    "\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 2s 707us/sample - loss: 0.2506 - categorical_accuracy: 0.9420\n",
      "3000/3000 [==============================] - 2s 639us/sample - loss: 1.1781 - categorical_accuracy: 0.6413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1781345659891764, 0.64133334]"
      ]
     },
     "execution_count": 1060,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_model.evaluate(target_data[0], target_data[1])\n",
    "target_model.evaluate(external_data[0], external_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Weight Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 20s 4ms/sample - loss: 1.9143 - categorical_accuracy: 0.2988 - val_loss: 1.8399 - val_categorical_accuracy: 0.3320\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 20s 4ms/sample - loss: 1.5754 - categorical_accuracy: 0.4304 - val_loss: 1.5265 - val_categorical_accuracy: 0.4442\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 1.4955 - categorical_accuracy: 0.4640 - val_loss: 1.4732 - val_categorical_accuracy: 0.4655\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 19s 4ms/sample - loss: 1.2837 - categorical_accuracy: 0.5402 - val_loss: 1.3521 - val_categorical_accuracy: 0.5272\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 24s 5ms/sample - loss: 1.3697 - categorical_accuracy: 0.5116 - val_loss: 1.2907 - val_categorical_accuracy: 0.5392\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 20s 4ms/sample - loss: 1.1642 - categorical_accuracy: 0.5800 - val_loss: 1.2964 - val_categorical_accuracy: 0.5420\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 21s 4ms/sample - loss: 1.1877 - categorical_accuracy: 0.5772 - val_loss: 1.2209 - val_categorical_accuracy: 0.5649\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 20s 4ms/sample - loss: 0.9629 - categorical_accuracy: 0.6654 - val_loss: 1.2081 - val_categorical_accuracy: 0.5728\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 20s 4ms/sample - loss: 1.0639 - categorical_accuracy: 0.6230 - val_loss: 1.2320 - val_categorical_accuracy: 0.5699\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.8299 - categorical_accuracy: 0.7094 - val_loss: 1.2622 - val_categorical_accuracy: 0.5738\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.9920 - categorical_accuracy: 0.6482 - val_loss: 1.1543 - val_categorical_accuracy: 0.6044\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.7197 - categorical_accuracy: 0.7482 - val_loss: 1.1896 - val_categorical_accuracy: 0.6089\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 0.8290 - categorical_accuracy: 0.7062 - val_loss: 1.1606 - val_categorical_accuracy: 0.6128\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.5255 - categorical_accuracy: 0.8238 - val_loss: 1.2666 - val_categorical_accuracy: 0.6093\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.7451 - categorical_accuracy: 0.7322 - val_loss: 1.2619 - val_categorical_accuracy: 0.6073\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.4273 - categorical_accuracy: 0.8482 - val_loss: 1.3376 - val_categorical_accuracy: 0.6030\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.6155 - categorical_accuracy: 0.7856 - val_loss: 1.2751 - val_categorical_accuracy: 0.6087\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.3027 - categorical_accuracy: 0.9042 - val_loss: 1.4120 - val_categorical_accuracy: 0.6064\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.4646 - categorical_accuracy: 0.8408 - val_loss: 1.4267 - val_categorical_accuracy: 0.5981\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.1990 - categorical_accuracy: 0.9398 - val_loss: 1.6950 - val_categorical_accuracy: 0.6005\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 21s 4ms/sample - loss: 0.4209 - categorical_accuracy: 0.8486 - val_loss: 1.5271 - val_categorical_accuracy: 0.6059\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 0.1564 - categorical_accuracy: 0.9532 - val_loss: 1.7434 - val_categorical_accuracy: 0.6112\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 0.3109 - categorical_accuracy: 0.8940 - val_loss: 1.6950 - val_categorical_accuracy: 0.6019\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.1008 - categorical_accuracy: 0.9730 - val_loss: 1.8165 - val_categorical_accuracy: 0.6208\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.2387 - categorical_accuracy: 0.9146 - val_loss: 1.8281 - val_categorical_accuracy: 0.5998\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.0883 - categorical_accuracy: 0.9742 - val_loss: 1.9204 - val_categorical_accuracy: 0.6217\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.2232 - categorical_accuracy: 0.9236 - val_loss: 1.9202 - val_categorical_accuracy: 0.6146\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 16s 3ms/sample - loss: 0.0773 - categorical_accuracy: 0.9772 - val_loss: 2.0779 - val_categorical_accuracy: 0.6020\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.2013 - categorical_accuracy: 0.9330 - val_loss: 2.1052 - val_categorical_accuracy: 0.5935\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.0603 - categorical_accuracy: 0.9842 - val_loss: 2.1921 - val_categorical_accuracy: 0.6097\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 22s 4ms/sample - loss: 0.1645 - categorical_accuracy: 0.9440 - val_loss: 2.1051 - val_categorical_accuracy: 0.5909\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 24s 5ms/sample - loss: 0.0465 - categorical_accuracy: 0.9870 - val_loss: 2.2197 - val_categorical_accuracy: 0.6161\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 25s 5ms/sample - loss: 0.1372 - categorical_accuracy: 0.9558 - val_loss: 2.4533 - val_categorical_accuracy: 0.5827\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 0.0516 - categorical_accuracy: 0.9850 - val_loss: 2.4119 - val_categorical_accuracy: 0.6132\n",
      "Train on 5000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.1585 - categorical_accuracy: 0.9488 - val_loss: 2.3296 - val_categorical_accuracy: 0.5914\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 18s 4ms/sample - loss: 0.0571 - categorical_accuracy: 0.9838 - val_loss: 2.4656 - val_categorical_accuracy: 0.6003\n"
     ]
    }
   ],
   "source": [
    "cyc_transfer_model = create_compiled_keras_model()\n",
    "CycTrTrain = []\n",
    "CycTrTest = []\n",
    "CycTrAcc = []\n",
    "\n",
    "for r in range(6):\n",
    "    for c in range(CLIENTS):\n",
    "        history_callback = cyc_transfer_model.fit(cifar_train_fed_data[c][0], cifar_train_fed_data[c][1], \n",
    "                                                     validation_data=cifar_test_data, batch_size=32, epochs=2)\n",
    "        CycTrTrain.append(history_callback.history['loss'])\n",
    "        CycTrTest.append(history_callback.history['val_loss'])\n",
    "        CycTrAcc.append(history_callback.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyc_transfer_model.save(\"models/weight_transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Cyclic Weight Transfer, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"CycTrTrain = {}\".format(CycTrTrain), file=log)\n",
    "        print(\"CyctTrTest = {}\".format(CycTrTest), file=log)\n",
    "        print(\"CycTrAccuracy = {}\".format(CycTrAcc), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "15000/15000 [==============================] - 12s 808us/sample - loss: 1.9066 - categorical_accuracy: 0.3631\n",
      "10000/10000 [==============================] - 7s 703us/sample - loss: 1.9095 - categorical_accuracy: 0.3568\n",
      "Epoch 2/18\n",
      "15000/15000 [==============================] - 11s 742us/sample - loss: 1.3897 - categorical_accuracy: 0.5039\n",
      "10000/10000 [==============================] - 9s 879us/sample - loss: 1.4220 - categorical_accuracy: 0.4875\n",
      "Epoch 3/18\n",
      "15000/15000 [==============================] - 12s 793us/sample - loss: 1.2819 - categorical_accuracy: 0.5447\n",
      "10000/10000 [==============================] - 8s 811us/sample - loss: 1.3366 - categorical_accuracy: 0.5230\n",
      "Epoch 4/18\n",
      "15000/15000 [==============================] - 10s 694us/sample - loss: 1.1373 - categorical_accuracy: 0.6055\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 1.2430 - categorical_accuracy: 0.5628\n",
      "Epoch 5/18\n",
      "15000/15000 [==============================] - 13s 873us/sample - loss: 1.0303 - categorical_accuracy: 0.6447\n",
      "10000/10000 [==============================] - 7s 708us/sample - loss: 1.1804 - categorical_accuracy: 0.5792\n",
      "Epoch 6/18\n",
      "15000/15000 [==============================] - 12s 797us/sample - loss: 0.9253 - categorical_accuracy: 0.6790\n",
      "10000/10000 [==============================] - 8s 808us/sample - loss: 1.1296 - categorical_accuracy: 0.5967\n",
      "Epoch 7/18\n",
      "15000/15000 [==============================] - 11s 746us/sample - loss: 0.8431 - categorical_accuracy: 0.7017\n",
      "10000/10000 [==============================] - 7s 683us/sample - loss: 1.1131 - categorical_accuracy: 0.6096\n",
      "Epoch 8/18\n",
      "15000/15000 [==============================] - 9s 614us/sample - loss: 0.7406 - categorical_accuracy: 0.7536\n",
      "10000/10000 [==============================] - 6s 585us/sample - loss: 1.0697 - categorical_accuracy: 0.6263\n",
      "Epoch 9/18\n",
      "15000/15000 [==============================] - 8s 554us/sample - loss: 0.6557 - categorical_accuracy: 0.7784\n",
      "10000/10000 [==============================] - 6s 558us/sample - loss: 1.0771 - categorical_accuracy: 0.6293\n",
      "Epoch 10/18\n",
      "15000/15000 [==============================] - 9s 602us/sample - loss: 0.5700 - categorical_accuracy: 0.8228\n",
      "10000/10000 [==============================] - 6s 552us/sample - loss: 1.0613 - categorical_accuracy: 0.6403\n",
      "Epoch 11/18\n",
      "15000/15000 [==============================] - 13s 837us/sample - loss: 0.4713 - categorical_accuracy: 0.8473\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.0833 - categorical_accuracy: 0.6473\n",
      "Epoch 12/18\n",
      "15000/15000 [==============================] - 11s 744us/sample - loss: 0.3758 - categorical_accuracy: 0.8843\n",
      "10000/10000 [==============================] - 7s 674us/sample - loss: 1.1039 - categorical_accuracy: 0.6515\n",
      "Epoch 13/18\n",
      "15000/15000 [==============================] - 11s 726us/sample - loss: 0.3177 - categorical_accuracy: 0.9060\n",
      "10000/10000 [==============================] - 7s 743us/sample - loss: 1.1513 - categorical_accuracy: 0.6525\n",
      "Epoch 14/18\n",
      "15000/15000 [==============================] - 8s 539us/sample - loss: 0.2506 - categorical_accuracy: 0.9291\n",
      "10000/10000 [==============================] - 5s 505us/sample - loss: 1.1920 - categorical_accuracy: 0.6480\n",
      "Epoch 15/18\n",
      "15000/15000 [==============================] - 8s 506us/sample - loss: 0.1970 - categorical_accuracy: 0.9543\n",
      "10000/10000 [==============================] - 5s 475us/sample - loss: 1.2290 - categorical_accuracy: 0.6486\n",
      "Epoch 16/18\n",
      "15000/15000 [==============================] - 7s 481us/sample - loss: 0.1524 - categorical_accuracy: 0.9635\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3136 - categorical_accuracy: 0.6429\n",
      "Epoch 17/18\n",
      "15000/15000 [==============================] - 9s 611us/sample - loss: 0.1108 - categorical_accuracy: 0.9768\n",
      "10000/10000 [==============================] - 7s 677us/sample - loss: 1.3856 - categorical_accuracy: 0.6417\n",
      "Epoch 18/18\n",
      "15000/15000 [==============================] - 12s 802us/sample - loss: 0.0911 - categorical_accuracy: 0.9818\n",
      "10000/10000 [==============================] - 6s 643us/sample - loss: 1.4673 - categorical_accuracy: 0.6418\n",
      "Epoch 19/18\n",
      "15000/15000 [==============================] - 12s 767us/sample - loss: 0.0644 - categorical_accuracy: 0.9871\n",
      "10000/10000 [==============================] - 5s 532us/sample - loss: 1.5878 - categorical_accuracy: 0.6427\n",
      "Epoch 20/18\n",
      "15000/15000 [==============================] - 8s 520us/sample - loss: 0.0615 - categorical_accuracy: 0.9865\n",
      "10000/10000 [==============================] - 5s 521us/sample - loss: 1.6887 - categorical_accuracy: 0.6381\n",
      "Epoch 21/18\n",
      "15000/15000 [==============================] - 9s 582us/sample - loss: 0.0527 - categorical_accuracy: 0.9905\n",
      "10000/10000 [==============================] - 6s 576us/sample - loss: 1.7536 - categorical_accuracy: 0.6325\n"
     ]
    }
   ],
   "source": [
    "initial_model = create_compiled_keras_model()\n",
    "\n",
    "FedTrain = []\n",
    "FedTest = []\n",
    "FedAcc = []\n",
    "\n",
    "for r in range(36):\n",
    "    \n",
    "    deltas = []\n",
    "\n",
    "    for c in range(CLIENTS):\n",
    "\n",
    "        federated_model = create_compiled_keras_model()\n",
    "\n",
    "        federated_model.set_weights(initial_model.get_weights())\n",
    "\n",
    "        history_callback = federated_model.fit(cifar_train_fed_data[c][0], cifar_train_fed_data[c][1], \n",
    "                                               batch_size=32, epochs=1, verbose=0)\n",
    "\n",
    "        \n",
    "        delta = np.array(initial_model.get_weights()) - np.array(federated_model.get_weights())\n",
    "\n",
    "        deltas.append(delta)\n",
    "    print('Epoch {}/18'.format(r+1))\n",
    "    delt_av = (deltas[0] + deltas[1] + deltas[2]) / 3\n",
    "    new_weights = np.array(initial_model.get_weights()) - delt_av\n",
    "    initial_model.set_weights(new_weights)\n",
    "    \n",
    "    FedTrain.append(initial_model.evaluate(cifar_train_data[0], cifar_train_data[1])[0])\n",
    "    validation = initial_model.evaluate(cifar_test_data[0], cifar_test_data[1])\n",
    "    FedTest.append(validation[0])\n",
    "    FedAcc.append(validation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_model.save(\"models/fedE1B32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated Model, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"FedTrainE1B32 = {}\".format(FedTrain), file=log)\n",
    "        print(\"FedTestE1B32 = {}\".format(FedTest), file=log)\n",
    "        print(\"FedAccuracyE1B32 = {}\".format(FedAcc), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attack (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outsider Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators=250, random_state=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/36\n",
      "5000/5000 [==============================] - 17s 3ms/sample - loss: 1.8887 - categorical_accuracy: 0.3086 - val_loss: 1.7002 - val_categorical_accuracy: 0.3774\n",
      "Epoch 2/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 1.5297 - categorical_accuracy: 0.4564 - val_loss: 1.5473 - val_categorical_accuracy: 0.4482\n",
      "Epoch 3/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 1.3551 - categorical_accuracy: 0.5150 - val_loss: 1.4526 - val_categorical_accuracy: 0.4782\n",
      "Epoch 4/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 1.1784 - categorical_accuracy: 0.5808 - val_loss: 1.3858 - val_categorical_accuracy: 0.5060\n",
      "Epoch 5/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.9826 - categorical_accuracy: 0.6508 - val_loss: 1.3590 - val_categorical_accuracy: 0.5230\n",
      "Epoch 6/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.8365 - categorical_accuracy: 0.7030 - val_loss: 1.3910 - val_categorical_accuracy: 0.5348\n",
      "Epoch 7/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.6392 - categorical_accuracy: 0.7822 - val_loss: 1.4248 - val_categorical_accuracy: 0.5454\n",
      "Epoch 8/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 0.4677 - categorical_accuracy: 0.8388 - val_loss: 1.6678 - val_categorical_accuracy: 0.5336\n",
      "Epoch 9/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.3172 - categorical_accuracy: 0.8972 - val_loss: 1.8018 - val_categorical_accuracy: 0.5368\n",
      "Epoch 10/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.1967 - categorical_accuracy: 0.9420 - val_loss: 2.0938 - val_categorical_accuracy: 0.5256\n",
      "Epoch 11/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.1276 - categorical_accuracy: 0.9620 - val_loss: 2.0904 - val_categorical_accuracy: 0.5398\n",
      "Epoch 12/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 0.0669 - categorical_accuracy: 0.9836 - val_loss: 2.5696 - val_categorical_accuracy: 0.5272\n",
      "Epoch 13/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0666 - categorical_accuracy: 0.9852 - val_loss: 2.5324 - val_categorical_accuracy: 0.5380\n",
      "Epoch 14/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.1159 - categorical_accuracy: 0.9614 - val_loss: 2.4760 - val_categorical_accuracy: 0.5354\n",
      "Epoch 15/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.0520 - categorical_accuracy: 0.9878 - val_loss: 2.6329 - val_categorical_accuracy: 0.5476\n",
      "Epoch 16/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 0.0229 - categorical_accuracy: 0.9964 - val_loss: 2.8733 - val_categorical_accuracy: 0.5528\n",
      "Epoch 17/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.0102 - categorical_accuracy: 0.9982 - val_loss: 2.9030 - val_categorical_accuracy: 0.5600\n",
      "Epoch 18/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 0.0258 - categorical_accuracy: 0.9924 - val_loss: 3.0568 - val_categorical_accuracy: 0.5342\n",
      "Epoch 19/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 0.0954 - categorical_accuracy: 0.9690 - val_loss: 3.0841 - val_categorical_accuracy: 0.5252\n",
      "Epoch 20/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 0.1072 - categorical_accuracy: 0.9642 - val_loss: 2.8776 - val_categorical_accuracy: 0.5270\n",
      "Epoch 21/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0528 - categorical_accuracy: 0.9830 - val_loss: 3.1427 - val_categorical_accuracy: 0.5196\n",
      "Epoch 22/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 0.0337 - categorical_accuracy: 0.9904 - val_loss: 2.9657 - val_categorical_accuracy: 0.5484\n",
      "Epoch 23/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0140 - categorical_accuracy: 0.9966 - val_loss: 3.2447 - val_categorical_accuracy: 0.5478\n",
      "Epoch 24/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 0.0027 - categorical_accuracy: 0.9996 - val_loss: 3.2433 - val_categorical_accuracy: 0.5478\n",
      "Epoch 25/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 0.0016 - categorical_accuracy: 0.9996 - val_loss: 3.3681 - val_categorical_accuracy: 0.5516\n",
      "Epoch 26/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 4.6294e-04 - categorical_accuracy: 1.0000 - val_loss: 3.4331 - val_categorical_accuracy: 0.5510\n",
      "Epoch 27/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 3.2788e-04 - categorical_accuracy: 1.0000 - val_loss: 3.4829 - val_categorical_accuracy: 0.5540\n",
      "Epoch 28/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 2.6628e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5273 - val_categorical_accuracy: 0.5540\n",
      "Epoch 29/36\n",
      "5000/5000 [==============================] - 12s 2ms/sample - loss: 2.2460e-04 - categorical_accuracy: 1.0000 - val_loss: 3.5771 - val_categorical_accuracy: 0.5536\n",
      "Epoch 30/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 1.9176e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6154 - val_categorical_accuracy: 0.5538\n",
      "Epoch 31/36\n",
      "5000/5000 [==============================] - 14s 3ms/sample - loss: 1.6553e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6583 - val_categorical_accuracy: 0.5536\n",
      "Epoch 32/36\n",
      "5000/5000 [==============================] - 13s 3ms/sample - loss: 1.4280e-04 - categorical_accuracy: 1.0000 - val_loss: 3.6940 - val_categorical_accuracy: 0.5536\n",
      "Epoch 33/36\n",
      "5000/5000 [==============================] - 15s 3ms/sample - loss: 1.2435e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7252 - val_categorical_accuracy: 0.5552\n",
      "Epoch 34/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 1.0679e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7641 - val_categorical_accuracy: 0.5552\n",
      "Epoch 35/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 9.4386e-05 - categorical_accuracy: 1.0000 - val_loss: 3.7950 - val_categorical_accuracy: 0.5560\n",
      "Epoch 36/36\n",
      "5000/5000 [==============================] - 11s 2ms/sample - loss: 8.2169e-05 - categorical_accuracy: 1.0000 - val_loss: 3.8311 - val_categorical_accuracy: 0.5570\n"
     ]
    }
   ],
   "source": [
    "# training a shadow model with external data\n",
    "shadow_model = create_compiled_keras_model()\n",
    "history_callback = shadow_model.fit(attacker_data[0], attacker_data[1], validation_data=external_data, batch_size=32, epochs=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_model.save('models/shadow_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = cifar_train_fed_data[0]\n",
    "target_model = tf.keras.models.load_model('models/fedE1B32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "in_preds = shadow_model.predict_proba(attacker_data)\n",
    "out_preds = shadow_model.predict_proba(external_data)\n",
    "\n",
    "        \n",
    "X_shadow = np.concatenate((np.concatenate((in_preds, out_preds), axis=0), \n",
    "                           np.concatenate((attacker_data[1], external_data[1]), axis=0)), \n",
    "                          axis=1)\n",
    "    \n",
    "y_shadow = np.concatenate((np.ones(SIZE), np.zeros(SIZE)), axis=0)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.79694061, 0.80233953, 0.79321729])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# amb = AdaBoostClassifier(n_estimators=250)\n",
    "# scores = cross_val_score(amb, X_shadow, y_shadow)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6521739130434783\n",
      "0.5765550239234449\n",
      "0.6944444444444444\n",
      "0.7829787234042553\n",
      "0.6919540229885057\n",
      "0.7123287671232876\n",
      "0.650887573964497\n",
      "0.595360824742268\n",
      "0.5928571428571429\n",
      "0.6239460370994941\n",
      "Average precision:  0.6184\n"
     ]
    }
   ],
   "source": [
    "# Compile them into the expected format for the AttackModelBundle.\n",
    "attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "    target_model, target_data, cifar_test_data)\n",
    "\n",
    "\n",
    "# Compute the attack accuracy.\n",
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_precision = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "\n",
    "class_precision = []\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(target_data[1].argmax(axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(cifar_test_data[1].argmax(axis=1)) if d == c]\n",
    "\n",
    "\n",
    "    print(np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                 + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "\n",
    "    class_precision.append(\n",
    "        np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                 + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "\n",
    "print('Average precision: ', attack_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/MIA_outsider_E3B64.txt', 'w') as log:\n",
    "        print(\"fedE3B64.h5 = {}\".format(class_precision), file=log)\n",
    "        #print(\"average_precision = {}\".format(attack_precision), file=log)\n",
    "\n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insider Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = cifar_train_fed_data[0]\n",
    "insider_attack_data = cifar_train_fed_data[2]\n",
    "\n",
    "target_model = tf.keras.models.load_model('models/fedE3B64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_preds = target_model.predict_proba(insider_attack_data)\n",
    "out_preds = target_model.predict_proba(external_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shadow = np.concatenate((np.concatenate((in_preds, out_preds), axis=0), \n",
    "                           np.concatenate((insider_attack_data[1], external_data[1]), axis=0)), axis=1)\n",
    "    \n",
    "y_shadow = np.concatenate((np.ones(15000), np.zeros(SIZE)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-272065088e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mamb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_shadow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_shadow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m attack_test_data, real_membership_labels = prepare_attack_data(target_model, \n\u001b[0;32m----> 8\u001b[0;31m                                                                target_data, cifar_test_data)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/mia/estimators.py\u001b[0m in \u001b[0;36mprepare_attack_data\u001b[0;34m(model, data_in, data_out)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattack\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \"\"\"\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mX_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mX_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0my_hat_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def attack_model_fn():\n",
    "    model = AdaBoostClassifier(n_estimators=250, random_state=0)  \n",
    "    return model\n",
    "\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "amb.fit(X_shadow, y_shadow)\n",
    "attack_test_data, real_membership_labels = prepare_attack_data(target_model, \n",
    "                                                               target_data, cifar_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6287878787878788\n",
      "0.5555555555555556\n",
      "0.6758620689655173\n",
      "0.6794871794871795\n",
      "0.6658767772511849\n",
      "0.6413502109704642\n",
      "0.5882352941176471\n",
      "0.5742049469964664\n",
      "0.5684931506849316\n",
      "0.5911179173047473\n",
      "Average precision:  0.6152\n"
     ]
    }
   ],
   "source": [
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_precision = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "\n",
    "class_precision = []\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(target_data[1].argmax(axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(cifar_test_data[1].argmax(axis=1)) if d == c]\n",
    "\n",
    "\n",
    "    print(np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                 + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "    class_precision.append(\n",
    "            np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                     + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "print(\"Average precision: \", attack_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/MIA_in_E3B64.txt', 'w') as log:\n",
    "        print(\"insider_E3B64 = {}\".format(class_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original MIA Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example membership inference attack against a deep net classifier on the CIFAR10 dataset\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 4000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer(\n",
    "    \"target_epochs\", 12, \"Number of epochs to train target and shadow models.\"\n",
    ")\n",
    "flags.DEFINE_integer(\"attack_epochs\", 12, \"Number of epochs to train attack models.\")\n",
    "flags.DEFINE_integer(\"num_shadows\", 3, \"Number of epochs to train attack models.\")\n",
    "\n",
    "\n",
    "# def get_data():\n",
    "#     \"\"\"Prepare CIFAR10 data.\"\"\"\n",
    "#     (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#     y_train = tf.keras.utils.to_categorical(y_train)\n",
    "#     y_test = tf.keras.utils.to_categorical(y_test)\n",
    "#     X_train = X_train.astype(\"float32\")[:5000]\n",
    "#     X_test = X_test.astype(\"float32\")\n",
    "#     y_train = y_train.astype(\"float32\") [:5000]\n",
    "#     y_test = y_test.astype(\"float32\")\n",
    "#     X_train /= 255\n",
    "#     X_test /= 255\n",
    "#     return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (5, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation=\"relu\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = get_data()\n",
    "\n",
    "\n",
    "# Train the target model.\n",
    "print(\"Training the target model...\")\n",
    "target_model = target_model_fn()\n",
    "target_model.fit(\n",
    "    X_train, y_train, epochs=FLAGS.target_epochs, validation_split=0.1, verbose=True\n",
    ")\n",
    "\n",
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=FLAGS.num_shadows,\n",
    ")\n",
    "\n",
    "# We assume that attacker's data were not seen in target's training.\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.1\n",
    ")\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=FLAGS.target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(\n",
    "    X_shadow, y_shadow, fit_kwargs=dict(epochs=FLAGS.attack_epochs, verbose=True)\n",
    ")\n",
    "\n",
    "# Test the success of the attack.\n",
    "\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
    "data_out = X_test[:ATTACK_TEST_DATASET_SIZE], y_test[:ATTACK_TEST_DATASET_SIZE]\n",
    "\n",
    "# Compile them into the expected format for the AttackModelBundle.\n",
    "attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "    target_model, data_in, data_out\n",
    ")\n",
    "\n",
    "# Compute the attack accuracy.\n",
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_tp = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "\n",
    "print(attack_accuracy)\n",
    "print (classification_report(real_membership_labels, attack_guesses))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "app.run(demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 5000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (5, 5),\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (5, 5), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation=\"relu\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('models/centralized3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 32, 32, 3) (2500, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3407 - accuracy: 0.1110 - val_loss: 2.2847 - val_accuracy: 0.1488\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.2218 - accuracy: 0.1710 - val_loss: 2.0921 - val_accuracy: 0.2516\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.9573 - accuracy: 0.2920 - val_loss: 1.8749 - val_accuracy: 0.3028\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.8120 - accuracy: 0.3430 - val_loss: 1.8169 - val_accuracy: 0.3652\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6613 - accuracy: 0.4010 - val_loss: 1.7489 - val_accuracy: 0.3652\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5246 - accuracy: 0.4480 - val_loss: 1.6567 - val_accuracy: 0.4124\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3476 - accuracy: 0.5230 - val_loss: 1.6901 - val_accuracy: 0.4120\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2361 - accuracy: 0.5650 - val_loss: 1.7116 - val_accuracy: 0.4056\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0964 - accuracy: 0.6060 - val_loss: 1.6676 - val_accuracy: 0.4392\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9333 - accuracy: 0.6670 - val_loss: 1.7117 - val_accuracy: 0.4212\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8218 - accuracy: 0.7210 - val_loss: 1.8905 - val_accuracy: 0.4104\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7148 - accuracy: 0.7560 - val_loss: 1.8596 - val_accuracy: 0.4284\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3131 - accuracy: 0.1010 - val_loss: 2.2570 - val_accuracy: 0.1740\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1453 - accuracy: 0.2100 - val_loss: 2.1530 - val_accuracy: 0.2248\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.9519 - accuracy: 0.2920 - val_loss: 1.9240 - val_accuracy: 0.3148\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7850 - accuracy: 0.3530 - val_loss: 1.8718 - val_accuracy: 0.3492\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6346 - accuracy: 0.3940 - val_loss: 1.8058 - val_accuracy: 0.3448\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5246 - accuracy: 0.4320 - val_loss: 1.7208 - val_accuracy: 0.3960\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3655 - accuracy: 0.5070 - val_loss: 1.7998 - val_accuracy: 0.3796\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.2726 - accuracy: 0.5490 - val_loss: 1.7034 - val_accuracy: 0.4052\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1512 - accuracy: 0.5800 - val_loss: 1.8292 - val_accuracy: 0.3836\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0225 - accuracy: 0.6440 - val_loss: 1.7070 - val_accuracy: 0.4220\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.8661 - accuracy: 0.6910 - val_loss: 1.8471 - val_accuracy: 0.4060\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7887 - accuracy: 0.7350 - val_loss: 1.9137 - val_accuracy: 0.4028\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3315 - accuracy: 0.1030 - val_loss: 2.2563 - val_accuracy: 0.1820\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1307 - accuracy: 0.2120 - val_loss: 2.0893 - val_accuracy: 0.2320\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.9108 - accuracy: 0.3300 - val_loss: 1.8804 - val_accuracy: 0.3432\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7477 - accuracy: 0.3770 - val_loss: 1.7845 - val_accuracy: 0.3740\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6217 - accuracy: 0.4090 - val_loss: 1.6978 - val_accuracy: 0.3888\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5077 - accuracy: 0.4480 - val_loss: 1.6789 - val_accuracy: 0.3800\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4446 - accuracy: 0.4950 - val_loss: 1.7261 - val_accuracy: 0.3840\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2730 - accuracy: 0.5380 - val_loss: 1.6294 - val_accuracy: 0.4188\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1617 - accuracy: 0.5860 - val_loss: 1.7869 - val_accuracy: 0.3608\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1658 - accuracy: 0.5740 - val_loss: 1.6855 - val_accuracy: 0.4092\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0095 - accuracy: 0.6400 - val_loss: 1.6488 - val_accuracy: 0.4288\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8687 - accuracy: 0.7010 - val_loss: 1.8666 - val_accuracy: 0.4104\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3390 - accuracy: 0.1280 - val_loss: 2.2829 - val_accuracy: 0.1480\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1950 - accuracy: 0.1700 - val_loss: 2.2018 - val_accuracy: 0.1748\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.9601 - accuracy: 0.3030 - val_loss: 1.9275 - val_accuracy: 0.3096\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.7212 - accuracy: 0.3930 - val_loss: 1.8135 - val_accuracy: 0.3540\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.6490 - accuracy: 0.4080 - val_loss: 1.7606 - val_accuracy: 0.3628\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.5105 - accuracy: 0.4660 - val_loss: 1.6665 - val_accuracy: 0.3992\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.4235 - accuracy: 0.5180 - val_loss: 1.6530 - val_accuracy: 0.4116\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.2955 - accuracy: 0.5360 - val_loss: 1.6938 - val_accuracy: 0.3848\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1888 - accuracy: 0.5700 - val_loss: 1.7203 - val_accuracy: 0.3916\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.0488 - accuracy: 0.6470 - val_loss: 1.7345 - val_accuracy: 0.4128\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.9450 - accuracy: 0.6670 - val_loss: 1.7355 - val_accuracy: 0.4100\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9007 - accuracy: 0.6660 - val_loss: 1.8425 - val_accuracy: 0.4032\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3537 - accuracy: 0.1060 - val_loss: 2.2923 - val_accuracy: 0.1484\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.2271 - accuracy: 0.1490 - val_loss: 2.1795 - val_accuracy: 0.2156\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.0585 - accuracy: 0.2510 - val_loss: 2.1406 - val_accuracy: 0.1804\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.0141 - accuracy: 0.2820 - val_loss: 2.0099 - val_accuracy: 0.2576\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.8382 - accuracy: 0.3460 - val_loss: 1.8284 - val_accuracy: 0.3492\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7692 - accuracy: 0.3670 - val_loss: 1.7819 - val_accuracy: 0.3596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6148 - accuracy: 0.4280 - val_loss: 1.7132 - val_accuracy: 0.3864\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.5188 - accuracy: 0.4400 - val_loss: 1.8195 - val_accuracy: 0.3496\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.4176 - accuracy: 0.5010 - val_loss: 1.6370 - val_accuracy: 0.4140\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2739 - accuracy: 0.5310 - val_loss: 1.7971 - val_accuracy: 0.3580\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1875 - accuracy: 0.5880 - val_loss: 1.7253 - val_accuracy: 0.3980\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.0809 - accuracy: 0.6290 - val_loss: 1.7084 - val_accuracy: 0.4272\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2772 - accuracy: 0.1480 - val_loss: 2.2014 - val_accuracy: 0.1636\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.0973 - accuracy: 0.2150 - val_loss: 2.1296 - val_accuracy: 0.2152\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.8903 - accuracy: 0.3100 - val_loss: 1.9417 - val_accuracy: 0.3036\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6879 - accuracy: 0.3780 - val_loss: 1.7300 - val_accuracy: 0.3596\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5141 - accuracy: 0.4650 - val_loss: 1.6559 - val_accuracy: 0.3928\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.3182 - accuracy: 0.5280 - val_loss: 1.6259 - val_accuracy: 0.4228\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2448 - accuracy: 0.5540 - val_loss: 1.6766 - val_accuracy: 0.3980\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.0565 - accuracy: 0.6260 - val_loss: 1.6827 - val_accuracy: 0.4152\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.0128 - accuracy: 0.6510 - val_loss: 1.7666 - val_accuracy: 0.4156\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.8134 - accuracy: 0.7210 - val_loss: 1.7827 - val_accuracy: 0.4260\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.6333 - accuracy: 0.7880 - val_loss: 1.9870 - val_accuracy: 0.4172\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.5154 - accuracy: 0.8220 - val_loss: 2.0547 - val_accuracy: 0.4052\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.3005 - accuracy: 0.1180 - val_loss: 2.1491 - val_accuracy: 0.2340\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1380 - accuracy: 0.2180 - val_loss: 2.0207 - val_accuracy: 0.2672\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.8453 - accuracy: 0.3370 - val_loss: 1.8566 - val_accuracy: 0.3336\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6224 - accuracy: 0.4230 - val_loss: 1.7470 - val_accuracy: 0.3768\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4668 - accuracy: 0.4720 - val_loss: 1.8661 - val_accuracy: 0.3480\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.3767 - accuracy: 0.5110 - val_loss: 1.7296 - val_accuracy: 0.3904\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2092 - accuracy: 0.5790 - val_loss: 1.7046 - val_accuracy: 0.4132\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.0524 - accuracy: 0.6410 - val_loss: 1.7304 - val_accuracy: 0.4332\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9400 - accuracy: 0.6750 - val_loss: 1.7692 - val_accuracy: 0.4260\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7768 - accuracy: 0.7270 - val_loss: 1.9120 - val_accuracy: 0.3988\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.6950 - accuracy: 0.7640 - val_loss: 1.9084 - val_accuracy: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.5871 - accuracy: 0.8000 - val_loss: 1.9581 - val_accuracy: 0.4176\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2614 - accuracy: 0.1620 - val_loss: 2.2223 - val_accuracy: 0.1572\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.0040 - accuracy: 0.2540 - val_loss: 1.9731 - val_accuracy: 0.2296\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.7523 - accuracy: 0.3620 - val_loss: 1.8386 - val_accuracy: 0.3236\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.6188 - accuracy: 0.4240 - val_loss: 1.6897 - val_accuracy: 0.3884\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 8s 8ms/sample - loss: 1.4322 - accuracy: 0.4720 - val_loss: 1.6650 - val_accuracy: 0.3968\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.2352 - accuracy: 0.5550 - val_loss: 1.7263 - val_accuracy: 0.3900\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1217 - accuracy: 0.5910 - val_loss: 1.6814 - val_accuracy: 0.4244\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9503 - accuracy: 0.6690 - val_loss: 1.7622 - val_accuracy: 0.4264\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8300 - accuracy: 0.6930 - val_loss: 1.8197 - val_accuracy: 0.4224\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6728 - accuracy: 0.7550 - val_loss: 1.9902 - val_accuracy: 0.3992\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5752 - accuracy: 0.7960 - val_loss: 1.9917 - val_accuracy: 0.4196\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4915 - accuracy: 0.8340 - val_loss: 2.0776 - val_accuracy: 0.4096\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2475 - accuracy: 0.1520 - val_loss: 2.1413 - val_accuracy: 0.1892\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.0117 - accuracy: 0.2760 - val_loss: 2.0122 - val_accuracy: 0.2468\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.8050 - accuracy: 0.3420 - val_loss: 1.9270 - val_accuracy: 0.3072\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6097 - accuracy: 0.4420 - val_loss: 1.7031 - val_accuracy: 0.3916\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4746 - accuracy: 0.4520 - val_loss: 1.7126 - val_accuracy: 0.4024\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3790 - accuracy: 0.5090 - val_loss: 1.6524 - val_accuracy: 0.4136\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2614 - accuracy: 0.5730 - val_loss: 1.7822 - val_accuracy: 0.3916\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.0944 - accuracy: 0.6210 - val_loss: 1.7664 - val_accuracy: 0.4080\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9760 - accuracy: 0.6680 - val_loss: 1.6937 - val_accuracy: 0.4228\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8253 - accuracy: 0.7140 - val_loss: 1.7908 - val_accuracy: 0.4192\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7458 - accuracy: 0.7480 - val_loss: 1.8228 - val_accuracy: 0.4404\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5721 - accuracy: 0.8160 - val_loss: 2.0790 - val_accuracy: 0.4228\n",
      "Train on 1000 samples, validate on 2500 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.2843 - accuracy: 0.1290 - val_loss: 2.0973 - val_accuracy: 0.2264\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1112 - accuracy: 0.2230 - val_loss: 2.0510 - val_accuracy: 0.2536\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.9629 - accuracy: 0.2930 - val_loss: 1.9078 - val_accuracy: 0.3500\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7956 - accuracy: 0.3760 - val_loss: 1.8294 - val_accuracy: 0.3444\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7277 - accuracy: 0.4040 - val_loss: 1.8243 - val_accuracy: 0.3336\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5349 - accuracy: 0.4420 - val_loss: 1.7493 - val_accuracy: 0.3816\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.3537 - accuracy: 0.5090 - val_loss: 1.6675 - val_accuracy: 0.4180\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1743 - accuracy: 0.5960 - val_loss: 1.6305 - val_accuracy: 0.4320\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.0099 - accuracy: 0.6550 - val_loss: 1.6990 - val_accuracy: 0.4068\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.8816 - accuracy: 0.7050 - val_loss: 1.6724 - val_accuracy: 0.4172\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7121 - accuracy: 0.7680 - val_loss: 1.8176 - val_accuracy: 0.4112\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.6191 - accuracy: 0.7840 - val_loss: 1.9126 - val_accuracy: 0.4392\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    cifar_test_data[0], cifar_test_data[1], test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Train on 2023 samples\n",
      "Epoch 1/12\n",
      "2023/2023 [==============================] - 1s 537us/sample - loss: 0.6256 - accuracy: 0.6698\n",
      "Epoch 2/12\n",
      "2023/2023 [==============================] - 0s 140us/sample - loss: 0.5717 - accuracy: 0.6990\n",
      "Epoch 3/12\n",
      "2023/2023 [==============================] - 0s 73us/sample - loss: 0.5654 - accuracy: 0.7000\n",
      "Epoch 4/12\n",
      "2023/2023 [==============================] - 0s 57us/sample - loss: 0.5624 - accuracy: 0.6980\n",
      "Epoch 5/12\n",
      "2023/2023 [==============================] - 0s 81us/sample - loss: 0.5641 - accuracy: 0.7054\n",
      "Epoch 6/12\n",
      "2023/2023 [==============================] - 0s 70us/sample - loss: 0.5580 - accuracy: 0.7009\n",
      "Epoch 7/12\n",
      "2023/2023 [==============================] - 0s 88us/sample - loss: 0.5527 - accuracy: 0.7084\n",
      "Epoch 8/12\n",
      "2023/2023 [==============================] - 0s 153us/sample - loss: 0.5548 - accuracy: 0.7019\n",
      "Epoch 9/12\n",
      "2023/2023 [==============================] - 0s 53us/sample - loss: 0.5488 - accuracy: 0.7093\n",
      "Epoch 10/12\n",
      "2023/2023 [==============================] - 0s 60us/sample - loss: 0.5511 - accuracy: 0.7128\n",
      "Epoch 11/12\n",
      "2023/2023 [==============================] - 0s 70us/sample - loss: 0.5491 - accuracy: 0.7158\n",
      "Epoch 12/12\n",
      "2023/2023 [==============================] - 0s 82us/sample - loss: 0.5477 - accuracy: 0.7163\n",
      "Train on 2008 samples\n",
      "Epoch 1/12\n",
      "2008/2008 [==============================] - 1s 273us/sample - loss: 0.6231 - accuracy: 0.6877\n",
      "Epoch 2/12\n",
      "2008/2008 [==============================] - 0s 68us/sample - loss: 0.5681 - accuracy: 0.7062\n",
      "Epoch 3/12\n",
      "2008/2008 [==============================] - 0s 131us/sample - loss: 0.5578 - accuracy: 0.7141\n",
      "Epoch 4/12\n",
      "2008/2008 [==============================] - 0s 81us/sample - loss: 0.5601 - accuracy: 0.7191\n",
      "Epoch 5/12\n",
      "2008/2008 [==============================] - 0s 74us/sample - loss: 0.5570 - accuracy: 0.7156\n",
      "Epoch 6/12\n",
      "2008/2008 [==============================] - 0s 51us/sample - loss: 0.5512 - accuracy: 0.7161\n",
      "Epoch 7/12\n",
      "2008/2008 [==============================] - 0s 48us/sample - loss: 0.5566 - accuracy: 0.7171\n",
      "Epoch 8/12\n",
      "2008/2008 [==============================] - 0s 48us/sample - loss: 0.5491 - accuracy: 0.7206\n",
      "Epoch 9/12\n",
      "2008/2008 [==============================] - 0s 48us/sample - loss: 0.5497 - accuracy: 0.7191\n",
      "Epoch 10/12\n",
      "2008/2008 [==============================] - 0s 56us/sample - loss: 0.5520 - accuracy: 0.7186\n",
      "Epoch 11/12\n",
      "2008/2008 [==============================] - 0s 66us/sample - loss: 0.5496 - accuracy: 0.7186\n",
      "Epoch 12/12\n",
      "2008/2008 [==============================] - 0s 55us/sample - loss: 0.5497 - accuracy: 0.7226\n",
      "Train on 2129 samples\n",
      "Epoch 1/12\n",
      "2129/2129 [==============================] - 1s 251us/sample - loss: 0.5829 - accuracy: 0.7078\n",
      "Epoch 2/12\n",
      "2129/2129 [==============================] - 0s 59us/sample - loss: 0.5140 - accuracy: 0.7515\n",
      "Epoch 3/12\n",
      "2129/2129 [==============================] - 0s 53us/sample - loss: 0.4970 - accuracy: 0.7586\n",
      "Epoch 4/12\n",
      "2129/2129 [==============================] - 0s 55us/sample - loss: 0.4926 - accuracy: 0.7680\n",
      "Epoch 5/12\n",
      "2129/2129 [==============================] - 0s 70us/sample - loss: 0.4886 - accuracy: 0.7553\n",
      "Epoch 6/12\n",
      "2129/2129 [==============================] - 0s 62us/sample - loss: 0.4850 - accuracy: 0.7619\n",
      "Epoch 7/12\n",
      "2129/2129 [==============================] - 0s 51us/sample - loss: 0.4841 - accuracy: 0.7680\n",
      "Epoch 8/12\n",
      "2129/2129 [==============================] - 0s 49us/sample - loss: 0.4829 - accuracy: 0.7647\n",
      "Epoch 9/12\n",
      "2129/2129 [==============================] - 0s 48us/sample - loss: 0.4838 - accuracy: 0.7675\n",
      "Epoch 10/12\n",
      "2129/2129 [==============================] - 0s 51us/sample - loss: 0.4778 - accuracy: 0.7684\n",
      "Epoch 11/12\n",
      "2129/2129 [==============================] - 0s 56us/sample - loss: 0.4807 - accuracy: 0.7666\n",
      "Epoch 12/12\n",
      "2129/2129 [==============================] - 0s 55us/sample - loss: 0.4767 - accuracy: 0.7703\n",
      "Train on 1926 samples\n",
      "Epoch 1/12\n",
      "1926/1926 [==============================] - 1s 269us/sample - loss: 0.5947 - accuracy: 0.7248\n",
      "Epoch 2/12\n",
      "1926/1926 [==============================] - 0s 73us/sample - loss: 0.5034 - accuracy: 0.7632\n",
      "Epoch 3/12\n",
      "1926/1926 [==============================] - 0s 59us/sample - loss: 0.4932 - accuracy: 0.7643\n",
      "Epoch 4/12\n",
      "1926/1926 [==============================] - 0s 66us/sample - loss: 0.4886 - accuracy: 0.7695\n",
      "Epoch 5/12\n",
      "1926/1926 [==============================] - 0s 64us/sample - loss: 0.4858 - accuracy: 0.7684\n",
      "Epoch 6/12\n",
      "1926/1926 [==============================] - 0s 90us/sample - loss: 0.4855 - accuracy: 0.7684\n",
      "Epoch 7/12\n",
      "1926/1926 [==============================] - 0s 77us/sample - loss: 0.4844 - accuracy: 0.7679\n",
      "Epoch 8/12\n",
      "1926/1926 [==============================] - 0s 63us/sample - loss: 0.4828 - accuracy: 0.7679\n",
      "Epoch 9/12\n",
      "1926/1926 [==============================] - 0s 56us/sample - loss: 0.4807 - accuracy: 0.7705\n",
      "Epoch 10/12\n",
      "1926/1926 [==============================] - 0s 67us/sample - loss: 0.4762 - accuracy: 0.7731\n",
      "Epoch 11/12\n",
      "1926/1926 [==============================] - 0s 56us/sample - loss: 0.4801 - accuracy: 0.7715\n",
      "Epoch 12/12\n",
      "1926/1926 [==============================] - 0s 74us/sample - loss: 0.4755 - accuracy: 0.7825\n",
      "Train on 2032 samples\n",
      "Epoch 1/12\n",
      "2032/2032 [==============================] - 1s 376us/sample - loss: 0.6132 - accuracy: 0.6983\n",
      "Epoch 2/12\n",
      "2032/2032 [==============================] - 0s 57us/sample - loss: 0.5365 - accuracy: 0.7367\n",
      "Epoch 3/12\n",
      "2032/2032 [==============================] - 0s 46us/sample - loss: 0.5289 - accuracy: 0.7352\n",
      "Epoch 4/12\n",
      "2032/2032 [==============================] - 0s 44us/sample - loss: 0.5245 - accuracy: 0.7367\n",
      "Epoch 5/12\n",
      "2032/2032 [==============================] - 0s 45us/sample - loss: 0.5190 - accuracy: 0.7436\n",
      "Epoch 6/12\n",
      "2032/2032 [==============================] - 0s 44us/sample - loss: 0.5177 - accuracy: 0.7451\n",
      "Epoch 7/12\n",
      "2032/2032 [==============================] - 0s 46us/sample - loss: 0.5140 - accuracy: 0.7392\n",
      "Epoch 8/12\n",
      "2032/2032 [==============================] - 0s 62us/sample - loss: 0.5104 - accuracy: 0.7397\n",
      "Epoch 9/12\n",
      "2032/2032 [==============================] - 0s 78us/sample - loss: 0.5045 - accuracy: 0.7485\n",
      "Epoch 10/12\n",
      "2032/2032 [==============================] - 0s 147us/sample - loss: 0.5113 - accuracy: 0.7480\n",
      "Epoch 11/12\n",
      "2032/2032 [==============================] - 0s 56us/sample - loss: 0.5031 - accuracy: 0.7530\n",
      "Epoch 12/12\n",
      "2032/2032 [==============================] - 0s 86us/sample - loss: 0.5086 - accuracy: 0.7451\n",
      "Train on 1939 samples\n",
      "Epoch 1/12\n",
      "1939/1939 [==============================] - 1s 325us/sample - loss: 0.5944 - accuracy: 0.7050\n",
      "Epoch 2/12\n",
      "1939/1939 [==============================] - 0s 68us/sample - loss: 0.5129 - accuracy: 0.7576\n",
      "Epoch 3/12\n",
      "1939/1939 [==============================] - 0s 59us/sample - loss: 0.5106 - accuracy: 0.7638\n",
      "Epoch 4/12\n",
      "1939/1939 [==============================] - 0s 50us/sample - loss: 0.4948 - accuracy: 0.7653\n",
      "Epoch 5/12\n",
      "1939/1939 [==============================] - 0s 49us/sample - loss: 0.4954 - accuracy: 0.7695\n",
      "Epoch 6/12\n",
      "1939/1939 [==============================] - 0s 50us/sample - loss: 0.4903 - accuracy: 0.7700\n",
      "Epoch 7/12\n",
      "1939/1939 [==============================] - 0s 82us/sample - loss: 0.4867 - accuracy: 0.7700\n",
      "Epoch 8/12\n",
      "1939/1939 [==============================] - 0s 78us/sample - loss: 0.4842 - accuracy: 0.7720\n",
      "Epoch 9/12\n",
      "1939/1939 [==============================] - 0s 50us/sample - loss: 0.4845 - accuracy: 0.7715\n",
      "Epoch 10/12\n",
      "1939/1939 [==============================] - 0s 48us/sample - loss: 0.4812 - accuracy: 0.7803\n",
      "Epoch 11/12\n",
      "1939/1939 [==============================] - 0s 60us/sample - loss: 0.4752 - accuracy: 0.7710\n",
      "Epoch 12/12\n",
      "1939/1939 [==============================] - 0s 57us/sample - loss: 0.4732 - accuracy: 0.7751\n",
      "Train on 1883 samples\n",
      "Epoch 1/12\n",
      "1883/1883 [==============================] - 8s 4ms/sample - loss: 0.6355 - accuracy: 0.6877\n",
      "Epoch 2/12\n",
      "1883/1883 [==============================] - 0s 57us/sample - loss: 0.5863 - accuracy: 0.6952\n",
      "Epoch 3/12\n",
      "1883/1883 [==============================] - 0s 49us/sample - loss: 0.5727 - accuracy: 0.7047\n",
      "Epoch 4/12\n",
      "1883/1883 [==============================] - 0s 49us/sample - loss: 0.5708 - accuracy: 0.6999\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1883/1883 [==============================] - 0s 47us/sample - loss: 0.5740 - accuracy: 0.6973\n",
      "Epoch 6/12\n",
      "1883/1883 [==============================] - 0s 47us/sample - loss: 0.5699 - accuracy: 0.6952\n",
      "Epoch 7/12\n",
      "1883/1883 [==============================] - 0s 46us/sample - loss: 0.5633 - accuracy: 0.7090\n",
      "Epoch 8/12\n",
      "1883/1883 [==============================] - 0s 47us/sample - loss: 0.5620 - accuracy: 0.7010\n",
      "Epoch 9/12\n",
      "1883/1883 [==============================] - 0s 47us/sample - loss: 0.5639 - accuracy: 0.6999\n",
      "Epoch 10/12\n",
      "1883/1883 [==============================] - 0s 44us/sample - loss: 0.5595 - accuracy: 0.6994\n",
      "Epoch 11/12\n",
      "1883/1883 [==============================] - 0s 44us/sample - loss: 0.5578 - accuracy: 0.7037\n",
      "Epoch 12/12\n",
      "1883/1883 [==============================] - 0s 61us/sample - loss: 0.5580 - accuracy: 0.7015\n",
      "Train on 1997 samples\n",
      "Epoch 1/12\n",
      "1997/1997 [==============================] - 1s 251us/sample - loss: 0.6324 - accuracy: 0.6755\n",
      "Epoch 2/12\n",
      "1997/1997 [==============================] - 0s 46us/sample - loss: 0.5833 - accuracy: 0.6970\n",
      "Epoch 3/12\n",
      "1997/1997 [==============================] - 0s 47us/sample - loss: 0.5786 - accuracy: 0.6975\n",
      "Epoch 4/12\n",
      "1997/1997 [==============================] - 0s 48us/sample - loss: 0.5731 - accuracy: 0.7051\n",
      "Epoch 5/12\n",
      "1997/1997 [==============================] - 0s 55us/sample - loss: 0.5714 - accuracy: 0.7036\n",
      "Epoch 6/12\n",
      "1997/1997 [==============================] - 0s 46us/sample - loss: 0.5702 - accuracy: 0.7051\n",
      "Epoch 7/12\n",
      "1997/1997 [==============================] - 0s 47us/sample - loss: 0.5654 - accuracy: 0.7031\n",
      "Epoch 8/12\n",
      "1997/1997 [==============================] - 0s 45us/sample - loss: 0.5638 - accuracy: 0.6995\n",
      "Epoch 9/12\n",
      "1997/1997 [==============================] - 0s 49us/sample - loss: 0.5697 - accuracy: 0.7031\n",
      "Epoch 10/12\n",
      "1997/1997 [==============================] - 0s 45us/sample - loss: 0.5613 - accuracy: 0.7046\n",
      "Epoch 11/12\n",
      "1997/1997 [==============================] - 0s 45us/sample - loss: 0.5644 - accuracy: 0.7101\n",
      "Epoch 12/12\n",
      "1997/1997 [==============================] - 0s 49us/sample - loss: 0.5608 - accuracy: 0.7041\n",
      "Train on 1920 samples\n",
      "Epoch 1/12\n",
      "1920/1920 [==============================] - 0s 214us/sample - loss: 0.6512 - accuracy: 0.6406\n",
      "Epoch 2/12\n",
      "1920/1920 [==============================] - 0s 39us/sample - loss: 0.6123 - accuracy: 0.6615\n",
      "Epoch 3/12\n",
      "1920/1920 [==============================] - 0s 41us/sample - loss: 0.6023 - accuracy: 0.6667\n",
      "Epoch 4/12\n",
      "1920/1920 [==============================] - 0s 41us/sample - loss: 0.5970 - accuracy: 0.6734\n",
      "Epoch 5/12\n",
      "1920/1920 [==============================] - 0s 39us/sample - loss: 0.5984 - accuracy: 0.6646\n",
      "Epoch 6/12\n",
      "1920/1920 [==============================] - 0s 41us/sample - loss: 0.5969 - accuracy: 0.6687\n",
      "Epoch 7/12\n",
      "1920/1920 [==============================] - 0s 43us/sample - loss: 0.5931 - accuracy: 0.6703\n",
      "Epoch 8/12\n",
      "1920/1920 [==============================] - 0s 41us/sample - loss: 0.5853 - accuracy: 0.6766\n",
      "Epoch 9/12\n",
      "1920/1920 [==============================] - 0s 41us/sample - loss: 0.5857 - accuracy: 0.6672\n",
      "Epoch 10/12\n",
      "1920/1920 [==============================] - 0s 43us/sample - loss: 0.5892 - accuracy: 0.6792\n",
      "Epoch 11/12\n",
      "1920/1920 [==============================] - 0s 51us/sample - loss: 0.5890 - accuracy: 0.6750\n",
      "Epoch 12/12\n",
      "1920/1920 [==============================] - 0s 44us/sample - loss: 0.5847 - accuracy: 0.6677\n",
      "Train on 2143 samples\n",
      "Epoch 1/12\n",
      "2143/2143 [==============================] - 0s 213us/sample - loss: 0.6097 - accuracy: 0.6869\n",
      "Epoch 2/12\n",
      "2143/2143 [==============================] - 0s 43us/sample - loss: 0.5598 - accuracy: 0.7200\n",
      "Epoch 3/12\n",
      "2143/2143 [==============================] - 0s 45us/sample - loss: 0.5479 - accuracy: 0.7219\n",
      "Epoch 4/12\n",
      "2143/2143 [==============================] - 0s 60us/sample - loss: 0.5466 - accuracy: 0.7242\n",
      "Epoch 5/12\n",
      "2143/2143 [==============================] - 0s 53us/sample - loss: 0.5443 - accuracy: 0.7242\n",
      "Epoch 6/12\n",
      "2143/2143 [==============================] - 0s 44us/sample - loss: 0.5434 - accuracy: 0.7228\n",
      "Epoch 7/12\n",
      "2143/2143 [==============================] - 0s 51us/sample - loss: 0.5404 - accuracy: 0.7196\n",
      "Epoch 8/12\n",
      "2143/2143 [==============================] - 0s 47us/sample - loss: 0.5427 - accuracy: 0.7261\n",
      "Epoch 9/12\n",
      "2143/2143 [==============================] - 0s 44us/sample - loss: 0.5383 - accuracy: 0.7233\n",
      "Epoch 10/12\n",
      "2143/2143 [==============================] - 0s 48us/sample - loss: 0.5398 - accuracy: 0.7284\n",
      "Epoch 11/12\n",
      "2143/2143 [==============================] - 0s 45us/sample - loss: 0.5381 - accuracy: 0.7224\n",
      "Epoch 12/12\n",
      "2143/2143 [==============================] - 0s 49us/sample - loss: 0.5334 - accuracy: 0.7252\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = tf.keras.models.load_model('models/centralized.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_test_data = (cifar_train[0][:15000]/255, tf.keras.utils.to_categorical(cifar_train[1][:15000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_test_data, real_membership_labels = prepare_attack_data(target_model, \n",
    "                                                               cifar_train_data, external_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5176817288801572\n",
      "0.4974721941354904\n",
      "0.47534516765285995\n",
      "0.4905094905094905\n",
      "0.49342770475227504\n",
      "0.5086119554204661\n",
      "0.5030181086519114\n",
      "0.4884422110552764\n",
      "0.4935323383084577\n",
      "0.529045643153527\n",
      "Average precision:  0.8483\n"
     ]
    }
   ],
   "source": [
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_precision = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "\n",
    "class_precision = []\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(target_data[1].argmax(axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(external_data[1].argmax(axis=1)) if d == c]\n",
    "\n",
    "\n",
    "    print(np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                 + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "    class_precision.append(\n",
    "            np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                     + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "print(\"Average precision: \", attack_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Logs/MIA_orig_E3B64.txt', 'w') as log:\n",
    "        print(\"orig_E3B64 = {}\".format(class_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = tf.keras.models.load_model('models/fedE3B64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_preds = target_model.predict_proba(insider_attack_data)\n",
    "out_preds = target_model.predict_proba(external_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shadow = np.concatenate((np.concatenate((in_preds, out_preds), axis=0), \n",
    "                           np.concatenate((insider_attack_data[1], external_data[1]), axis=0)), axis=1)\n",
    "    \n",
    "y_shadow = np.concatenate((np.ones(SIZE), np.zeros(SIZE)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Train on 991 samples\n",
      "Epoch 1/12\n",
      "991/991 [==============================] - 1s 1ms/sample - loss: 0.6608 - accuracy: 0.6650\n",
      "Epoch 2/12\n",
      "991/991 [==============================] - 0s 77us/sample - loss: 0.5781 - accuracy: 0.7124\n",
      "Epoch 3/12\n",
      "991/991 [==============================] - 0s 57us/sample - loss: 0.5521 - accuracy: 0.7195\n",
      "Epoch 4/12\n",
      "991/991 [==============================] - 0s 60us/sample - loss: 0.5364 - accuracy: 0.7245\n",
      "Epoch 5/12\n",
      "991/991 [==============================] - 0s 66us/sample - loss: 0.5434 - accuracy: 0.7205\n",
      "Epoch 6/12\n",
      "991/991 [==============================] - 0s 52us/sample - loss: 0.5329 - accuracy: 0.7265\n",
      "Epoch 7/12\n",
      "991/991 [==============================] - 0s 53us/sample - loss: 0.5387 - accuracy: 0.7326\n",
      "Epoch 8/12\n",
      "991/991 [==============================] - 0s 58us/sample - loss: 0.5289 - accuracy: 0.7265\n",
      "Epoch 9/12\n",
      "991/991 [==============================] - 0s 68us/sample - loss: 0.5308 - accuracy: 0.7316\n",
      "Epoch 10/12\n",
      "991/991 [==============================] - 0s 54us/sample - loss: 0.5329 - accuracy: 0.7286\n",
      "Epoch 11/12\n",
      "991/991 [==============================] - 0s 57us/sample - loss: 0.5268 - accuracy: 0.7397\n",
      "Epoch 12/12\n",
      "991/991 [==============================] - 0s 62us/sample - loss: 0.5303 - accuracy: 0.7265\n",
      "Train on 1031 samples\n",
      "Epoch 1/12\n",
      "1031/1031 [==============================] - 1s 697us/sample - loss: 0.6645 - accuracy: 0.6460\n",
      "Epoch 2/12\n",
      "1031/1031 [==============================] - 0s 67us/sample - loss: 0.5898 - accuracy: 0.6906\n",
      "Epoch 3/12\n",
      "1031/1031 [==============================] - 0s 131us/sample - loss: 0.5607 - accuracy: 0.7032\n",
      "Epoch 4/12\n",
      "1031/1031 [==============================] - 0s 73us/sample - loss: 0.5559 - accuracy: 0.7042\n",
      "Epoch 5/12\n",
      "1031/1031 [==============================] - 0s 54us/sample - loss: 0.5510 - accuracy: 0.7061\n",
      "Epoch 6/12\n",
      "1031/1031 [==============================] - 0s 58us/sample - loss: 0.5532 - accuracy: 0.7051\n",
      "Epoch 7/12\n",
      "1031/1031 [==============================] - 0s 73us/sample - loss: 0.5555 - accuracy: 0.7061\n",
      "Epoch 8/12\n",
      "1031/1031 [==============================] - 0s 68us/sample - loss: 0.5518 - accuracy: 0.7051\n",
      "Epoch 9/12\n",
      "1031/1031 [==============================] - 0s 51us/sample - loss: 0.5489 - accuracy: 0.7061\n",
      "Epoch 10/12\n",
      "1031/1031 [==============================] - 0s 49us/sample - loss: 0.5487 - accuracy: 0.7100\n",
      "Epoch 11/12\n",
      "1031/1031 [==============================] - 0s 45us/sample - loss: 0.5470 - accuracy: 0.7119\n",
      "Epoch 12/12\n",
      "1031/1031 [==============================] - 0s 53us/sample - loss: 0.5424 - accuracy: 0.7100\n",
      "Train on 995 samples\n",
      "Epoch 1/12\n",
      "995/995 [==============================] - 1s 557us/sample - loss: 0.6305 - accuracy: 0.7377\n",
      "Epoch 2/12\n",
      "995/995 [==============================] - 0s 68us/sample - loss: 0.4733 - accuracy: 0.7960\n",
      "Epoch 3/12\n",
      "995/995 [==============================] - 0s 61us/sample - loss: 0.4389 - accuracy: 0.8040\n",
      "Epoch 4/12\n",
      "995/995 [==============================] - 0s 51us/sample - loss: 0.4374 - accuracy: 0.8121\n",
      "Epoch 5/12\n",
      "995/995 [==============================] - 0s 79us/sample - loss: 0.4299 - accuracy: 0.8101\n",
      "Epoch 6/12\n",
      "995/995 [==============================] - 0s 62us/sample - loss: 0.4265 - accuracy: 0.8101\n",
      "Epoch 7/12\n",
      "995/995 [==============================] - 0s 55us/sample - loss: 0.4196 - accuracy: 0.8131\n",
      "Epoch 8/12\n",
      "995/995 [==============================] - 0s 65us/sample - loss: 0.4138 - accuracy: 0.8151\n",
      "Epoch 9/12\n",
      "995/995 [==============================] - 0s 65us/sample - loss: 0.4227 - accuracy: 0.8101\n",
      "Epoch 10/12\n",
      "995/995 [==============================] - 0s 57us/sample - loss: 0.4186 - accuracy: 0.8101\n",
      "Epoch 11/12\n",
      "995/995 [==============================] - 0s 49us/sample - loss: 0.4163 - accuracy: 0.8141\n",
      "Epoch 12/12\n",
      "995/995 [==============================] - 0s 54us/sample - loss: 0.4171 - accuracy: 0.8161\n",
      "Train on 1006 samples\n",
      "Epoch 1/12\n",
      "1006/1006 [==============================] - 1s 576us/sample - loss: 0.6050 - accuracy: 0.7654\n",
      "Epoch 2/12\n",
      "1006/1006 [==============================] - 0s 81us/sample - loss: 0.4368 - accuracy: 0.8330\n",
      "Epoch 3/12\n",
      "1006/1006 [==============================] - 0s 69us/sample - loss: 0.4041 - accuracy: 0.8310\n",
      "Epoch 4/12\n",
      "1006/1006 [==============================] - 0s 50us/sample - loss: 0.4069 - accuracy: 0.8390\n",
      "Epoch 5/12\n",
      "1006/1006 [==============================] - 0s 49us/sample - loss: 0.4043 - accuracy: 0.8390\n",
      "Epoch 6/12\n",
      "1006/1006 [==============================] - 0s 48us/sample - loss: 0.4024 - accuracy: 0.8370\n",
      "Epoch 7/12\n",
      "1006/1006 [==============================] - 0s 56us/sample - loss: 0.3988 - accuracy: 0.8390\n",
      "Epoch 8/12\n",
      "1006/1006 [==============================] - 0s 47us/sample - loss: 0.3867 - accuracy: 0.8410\n",
      "Epoch 9/12\n",
      "1006/1006 [==============================] - 0s 47us/sample - loss: 0.3905 - accuracy: 0.8380\n",
      "Epoch 10/12\n",
      "1006/1006 [==============================] - 0s 47us/sample - loss: 0.3874 - accuracy: 0.8370\n",
      "Epoch 11/12\n",
      "1006/1006 [==============================] - 0s 48us/sample - loss: 0.3903 - accuracy: 0.8400\n",
      "Epoch 12/12\n",
      "1006/1006 [==============================] - 0s 76us/sample - loss: 0.3872 - accuracy: 0.8390\n",
      "Train on 1023 samples\n",
      "Epoch 1/12\n",
      "1023/1023 [==============================] - 1s 561us/sample - loss: 0.6532 - accuracy: 0.6540\n",
      "Epoch 2/12\n",
      "1023/1023 [==============================] - 0s 53us/sample - loss: 0.5308 - accuracy: 0.7654\n",
      "Epoch 3/12\n",
      "1023/1023 [==============================] - 0s 48us/sample - loss: 0.4831 - accuracy: 0.7801\n",
      "Epoch 4/12\n",
      "1023/1023 [==============================] - 0s 70us/sample - loss: 0.4714 - accuracy: 0.7722\n",
      "Epoch 5/12\n",
      "1023/1023 [==============================] - 0s 53us/sample - loss: 0.4708 - accuracy: 0.7781\n",
      "Epoch 6/12\n",
      "1023/1023 [==============================] - 0s 48us/sample - loss: 0.4598 - accuracy: 0.7820\n",
      "Epoch 7/12\n",
      "1023/1023 [==============================] - 0s 66us/sample - loss: 0.4620 - accuracy: 0.7830\n",
      "Epoch 8/12\n",
      "1023/1023 [==============================] - 0s 95us/sample - loss: 0.4622 - accuracy: 0.7859\n",
      "Epoch 9/12\n",
      "1023/1023 [==============================] - 0s 79us/sample - loss: 0.4634 - accuracy: 0.7849\n",
      "Epoch 10/12\n",
      "1023/1023 [==============================] - 0s 96us/sample - loss: 0.4594 - accuracy: 0.7859\n",
      "Epoch 11/12\n",
      "1023/1023 [==============================] - 0s 144us/sample - loss: 0.4572 - accuracy: 0.7869\n",
      "Epoch 12/12\n",
      "1023/1023 [==============================] - 0s 85us/sample - loss: 0.4500 - accuracy: 0.7849\n",
      "Train on 978 samples\n",
      "Epoch 1/12\n",
      "978/978 [==============================] - 1s 543us/sample - loss: 0.6432 - accuracy: 0.7178\n",
      "Epoch 2/12\n",
      "978/978 [==============================] - 0s 83us/sample - loss: 0.5260 - accuracy: 0.7607\n",
      "Epoch 3/12\n",
      "978/978 [==============================] - 0s 59us/sample - loss: 0.4773 - accuracy: 0.7730\n",
      "Epoch 4/12\n",
      "978/978 [==============================] - 0s 68us/sample - loss: 0.4671 - accuracy: 0.7771\n",
      "Epoch 5/12\n",
      "978/978 [==============================] - 0s 61us/sample - loss: 0.4608 - accuracy: 0.7771\n",
      "Epoch 6/12\n",
      "978/978 [==============================] - 0s 86us/sample - loss: 0.4622 - accuracy: 0.7853\n",
      "Epoch 7/12\n",
      "978/978 [==============================] - 0s 73us/sample - loss: 0.4595 - accuracy: 0.7853\n",
      "Epoch 8/12\n",
      "978/978 [==============================] - 0s 70us/sample - loss: 0.4571 - accuracy: 0.7812\n",
      "Epoch 9/12\n",
      "978/978 [==============================] - 0s 75us/sample - loss: 0.4587 - accuracy: 0.7853\n",
      "Epoch 10/12\n",
      "978/978 [==============================] - 0s 74us/sample - loss: 0.4491 - accuracy: 0.7853\n",
      "Epoch 11/12\n",
      "978/978 [==============================] - 0s 70us/sample - loss: 0.4507 - accuracy: 0.7863\n",
      "Epoch 12/12\n",
      "978/978 [==============================] - 0s 71us/sample - loss: 0.4512 - accuracy: 0.7853\n",
      "Train on 977 samples\n",
      "Epoch 1/12\n",
      "977/977 [==============================] - 1s 690us/sample - loss: 0.6845 - accuracy: 0.5752\n",
      "Epoch 2/12\n",
      "977/977 [==============================] - 0s 92us/sample - loss: 0.6524 - accuracy: 0.6131\n",
      "Epoch 3/12\n",
      "977/977 [==============================] - 0s 121us/sample - loss: 0.6229 - accuracy: 0.5988\n",
      "Epoch 4/12\n",
      "977/977 [==============================] - 0s 132us/sample - loss: 0.5995 - accuracy: 0.6407\n",
      "Epoch 5/12\n",
      "977/977 [==============================] - 0s 103us/sample - loss: 0.5884 - accuracy: 0.6571\n",
      "Epoch 6/12\n",
      "977/977 [==============================] - 0s 91us/sample - loss: 0.5776 - accuracy: 0.6551\n",
      "Epoch 7/12\n",
      "977/977 [==============================] - 0s 76us/sample - loss: 0.5799 - accuracy: 0.6469\n",
      "Epoch 8/12\n",
      "977/977 [==============================] - 0s 54us/sample - loss: 0.5755 - accuracy: 0.6592\n",
      "Epoch 9/12\n",
      "977/977 [==============================] - 0s 97us/sample - loss: 0.5711 - accuracy: 0.6786\n",
      "Epoch 10/12\n",
      "977/977 [==============================] - 0s 52us/sample - loss: 0.5715 - accuracy: 0.6694\n",
      "Epoch 11/12\n",
      "977/977 [==============================] - 0s 46us/sample - loss: 0.5717 - accuracy: 0.6643\n",
      "Epoch 12/12\n",
      "977/977 [==============================] - 0s 51us/sample - loss: 0.5624 - accuracy: 0.6940\n",
      "Train on 987 samples\n",
      "Epoch 1/12\n",
      "987/987 [==============================] - 1s 819us/sample - loss: 0.6737 - accuracy: 0.5816\n",
      "Epoch 2/12\n",
      "987/987 [==============================] - 0s 78us/sample - loss: 0.5973 - accuracy: 0.6879\n",
      "Epoch 3/12\n",
      "987/987 [==============================] - 0s 68us/sample - loss: 0.5639 - accuracy: 0.6859\n",
      "Epoch 4/12\n",
      "987/987 [==============================] - 0s 286us/sample - loss: 0.5578 - accuracy: 0.6920\n",
      "Epoch 5/12\n",
      "987/987 [==============================] - 0s 97us/sample - loss: 0.5584 - accuracy: 0.6930\n",
      "Epoch 6/12\n",
      "987/987 [==============================] - 0s 98us/sample - loss: 0.5584 - accuracy: 0.6930\n",
      "Epoch 7/12\n",
      "987/987 [==============================] - 0s 89us/sample - loss: 0.5539 - accuracy: 0.6920\n",
      "Epoch 8/12\n",
      "987/987 [==============================] - 0s 93us/sample - loss: 0.5499 - accuracy: 0.6910\n",
      "Epoch 9/12\n",
      "987/987 [==============================] - 0s 61us/sample - loss: 0.5484 - accuracy: 0.6930\n",
      "Epoch 10/12\n",
      "987/987 [==============================] - 0s 46us/sample - loss: 0.5526 - accuracy: 0.6950\n",
      "Epoch 11/12\n",
      "987/987 [==============================] - 0s 65us/sample - loss: 0.5472 - accuracy: 0.6950\n",
      "Epoch 12/12\n",
      "987/987 [==============================] - 0s 47us/sample - loss: 0.5487 - accuracy: 0.6960\n",
      "Train on 1015 samples\n",
      "Epoch 1/12\n",
      "1015/1015 [==============================] - 1s 830us/sample - loss: 0.6784 - accuracy: 0.5626\n",
      "Epoch 2/12\n",
      "1015/1015 [==============================] - 0s 82us/sample - loss: 0.6434 - accuracy: 0.6266\n",
      "Epoch 3/12\n",
      "1015/1015 [==============================] - 0s 125us/sample - loss: 0.6271 - accuracy: 0.6069\n",
      "Epoch 4/12\n",
      "1015/1015 [==============================] - 0s 72us/sample - loss: 0.6230 - accuracy: 0.6138\n",
      "Epoch 5/12\n",
      "1015/1015 [==============================] - 0s 143us/sample - loss: 0.6154 - accuracy: 0.6187\n",
      "Epoch 6/12\n",
      "1015/1015 [==============================] - 0s 79us/sample - loss: 0.6191 - accuracy: 0.6286\n",
      "Epoch 7/12\n",
      "1015/1015 [==============================] - 0s 70us/sample - loss: 0.6209 - accuracy: 0.6236\n",
      "Epoch 8/12\n",
      "1015/1015 [==============================] - 0s 55us/sample - loss: 0.6177 - accuracy: 0.6266\n",
      "Epoch 9/12\n",
      "1015/1015 [==============================] - 0s 67us/sample - loss: 0.6140 - accuracy: 0.6227\n",
      "Epoch 10/12\n",
      "1015/1015 [==============================] - 0s 52us/sample - loss: 0.6169 - accuracy: 0.6217\n",
      "Epoch 11/12\n",
      "1015/1015 [==============================] - 0s 46us/sample - loss: 0.6150 - accuracy: 0.6177\n",
      "Epoch 12/12\n",
      "1015/1015 [==============================] - 0s 52us/sample - loss: 0.6123 - accuracy: 0.6266\n",
      "Train on 997 samples\n",
      "Epoch 1/12\n",
      "997/997 [==============================] - 1s 687us/sample - loss: 0.6729 - accuracy: 0.5757\n",
      "Epoch 2/12\n",
      "997/997 [==============================] - 0s 91us/sample - loss: 0.6373 - accuracy: 0.6780\n",
      "Epoch 3/12\n",
      "997/997 [==============================] - 0s 79us/sample - loss: 0.5927 - accuracy: 0.6941\n",
      "Epoch 4/12\n",
      "997/997 [==============================] - 0s 64us/sample - loss: 0.5722 - accuracy: 0.7011\n",
      "Epoch 5/12\n",
      "997/997 [==============================] - 0s 77us/sample - loss: 0.5766 - accuracy: 0.6881\n",
      "Epoch 6/12\n",
      "997/997 [==============================] - 0s 65us/sample - loss: 0.5747 - accuracy: 0.6981\n",
      "Epoch 7/12\n",
      "997/997 [==============================] - 0s 86us/sample - loss: 0.5640 - accuracy: 0.6981\n",
      "Epoch 8/12\n",
      "997/997 [==============================] - 0s 66us/sample - loss: 0.5600 - accuracy: 0.7061\n",
      "Epoch 9/12\n",
      "997/997 [==============================] - 0s 57us/sample - loss: 0.5597 - accuracy: 0.7071\n",
      "Epoch 10/12\n",
      "997/997 [==============================] - 0s 71us/sample - loss: 0.5630 - accuracy: 0.7081\n",
      "Epoch 11/12\n",
      "997/997 [==============================] - 0s 71us/sample - loss: 0.5617 - accuracy: 0.7061\n",
      "Epoch 12/12\n",
      "997/997 [==============================] - 0s 56us/sample - loss: 0.5598 - accuracy: 0.7081\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6224648985959438\n",
      "0.5595408895265424\n",
      "0.6800804828973843\n",
      "0.707196029776675\n",
      "0.650887573964497\n",
      "0.6438127090301003\n",
      "0.5837765957446809\n",
      "0.5888399412628488\n",
      "0.5659121171770972\n",
      "0.5822784810126582\n",
      "Average precision:  0.6387\n"
     ]
    }
   ],
   "source": [
    "attack_test_data, real_membership_labels = prepare_attack_data(target_model, target_data, cifar_test_data)\n",
    "\n",
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_precision = np.mean((attack_guesses == 1) == (real_membership_labels == 1))\n",
    "\n",
    "class_precision = []\n",
    "\n",
    "for c in range(NUM_CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(target_data[1].argmax(axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(cifar_test_data[1].argmax(axis=1)) if d == c]\n",
    "\n",
    "\n",
    "    print(np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                 + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "    class_precision.append(\n",
    "            np.sum(attack_guesses[target_indices]==1) / (np.sum(attack_guesses[target_indices])\n",
    "                                                     + np.sum(attack_guesses[SIZE:][test_indices])))\n",
    "print(\"Average precision: \", attack_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
